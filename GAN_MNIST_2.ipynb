{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GAN\n",
        "\n",
        "## <코드1> 라이브러리 및 데이터 불러오기\n",
        "\n",
        "### Ref : https://dreamgonfly.github.io/blog/gan-explained/#gan-직접-만들어보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#데이터 전처리 방식을 지정한다.\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(), # 데이터를 파이토치의 Tensor 형식으로바꾼다.\n",
        "  transforms.Normalize(mean=(0.5,), std=(0.5,)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n",
        "])\n",
        "\n",
        "#MNIST 데이터셋을 불러온다. 지정한 폴더에 없을 경우 자동으로 다운로드한다.\n",
        "mnist =datasets.MNIST(root='dataset', download=True, transform=transform)\n",
        "\n",
        "#데이터를 한번에 batch_size만큼만 가져오는 dataloader를 만든다.\n",
        "dataloader =DataLoader(mnist, batch_size=60, shuffle=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GAN의 2가지 요소인 생성자와 구분자 중 생성자(Generator)를 먼저 만들어보자. 생성자는 랜덤 벡터 ‘z’를 입력으로 받아 가짜 이미지를 출력하는 함수다. 여기서 ‘z’는 단순하게 균등 분포(Uniform Distribution)나 정규 분포(Normal Distribution)에서 무작위로 추출된 값이다. 생성자는 이렇게 단순한 분포를 사람 얼굴 이미지와 같은 복잡한 분포로 매핑(Mapping)하는 함수라고 볼 수 있다. 생성자 모델에 충분한 수의 매개 변수가 있다면 어떤 복잡한 분포도 근사할 수 있다는 것이 알려져 있다.\n",
        "\n",
        "![](https://lens.google.com/search?ep=gsbubb&hl=ko&re=df&p=Acn1BYcI4aRioulCGXQkQr1HS88i8-_c-JCYFrykNJ28VitEqACmwjlIYAaCISug28cP6PX-krfDzfkOWIhRDb9otivAu5iNMblCogly19kMuGXBb78-PIzMlkYjPmsyV_kWc8aQGBYCunsCYHRSeS0FkD_cEDhkK8k2m7AF6IdWSl7ViTTO38iu7CNOWp2Zk-ajtpWF4FYSPciNFCPOCwQp#lns=W251bGwsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsIkVrY0tKREV3TXprd01HVXhMVEkxWkRjdE5EUXpNaTA1TXpkakxUY3hPREV6WmpsaE1HTmxNUklmU1hsdE1FRkVSMnhRUkhkUmQwMURZVjlYYUdaVlQxVjFTMmQxWld0U1p3PT0iXQ==)\n",
        "\n",
        "‘z’ 벡터가 존재하는 공간을 잠재 공간(Latent Space)이라고도 부른다. 여기서는 잠재 공간의 크기를 임의로 100차원으로 뒀다. 잠재 공간의 크기에는 제한이 없으나 나타내려고 하는 대상의 정보를 충분히 담을 수 있을 만큼은 커야 한다. GAN은 우리가 이해할 수는 없는 방식이지만 ‘z’ 벡터의 값을 이미지의 속성에 매핑시키기 때문이다. 뒤에 살펴볼 GAN의 파생 모델에서 잠재 공간의 의미를 더욱 자세히 이해할 수 있을 것이다.\n",
        "\n",
        "생성자에 충분한 수의 매개 변수를 확보하기 위해, 이 구현에서는 4개의 선형 레이어(Linear Layer, Fully Connected Layer, Linear Transformation)를 쌓아서 생성자를 만들었다. 선형 레이어는 속해있는 모든 뉴런이 이전 레이어의 모든 뉴런과 연결되는 가장 단순한 구조의 레이어다. 이 모델에서는 100차원의 랜덤 벡터를 받아 이를 256개의 뉴런을 가진 레이어로 보내고, 다시 레이어의 크기를 512, 1024로 점점 증가시켰다. 마지막에는 출력을 MNIST 이미지의 크기로 맞추기 위해 레이어 크기를 28x28로 줄였다.\n",
        "\n",
        "각 레이어마다 활성 함수로는 LeakyReLU를 이용했다. LeakyReLU는 각 뉴런의 출력값이 0보다 높으면 그대로 놔두고, 0보다 낮으면 정해진 작은 숫자를 곱하는 간단한 함수다. 여기서는 0.2를 곱했다. 이밖에도 활성 함수로는 ReLU, Elu, Tanh, Sigmoid 등이 자주 쓰인다. 생성자의 마지막 레이어에서는 출력값을 픽셀값의 범위인 -1과 1 사이로 만들어주기 위해 Tanh를 사용했다.\n",
        "\n",
        "이렇게 여러 개의 레이어와 활성 함수를 쌓은 덕분에 MNIST의 데이터 분포를 근사할 수 있는 충분한 표현력(Representation Power)을 얻을 수 있었다. MNIST는 비교적 간단한 문제에 속하므로 더욱 복잡한 문제를 풀기 위해서는 더 깊은 레이어 구조와 더 많은 양의 매개 변수가 필요할 것이다.\n",
        "\n",
        "## # <코드2> GAN의 생성자(Generator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n",
        "class Generator(nn.Module):\n",
        "    # 네트워크 구조\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=100, out_features=256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(in_features=256, out_features=512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(in_features=1024, out_features=28 * 28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    # (batch_size x 100) 크기의 랜덤 벡터를 받아 이미지를 (batch_size x 1 x 28 x 28)크기로 출력한다.\n",
        "    def forward(self, inputs):\n",
        "        return self.main(inputs).view(-1, 1, 28, 28)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "구분자는 이미지를 입력으로 받고 그 이미지가 진짜일 확률을 0과 1 사이의 숫자 하나로 출력하는 함수다. 구분자의 구현은 생성자와 마찬가지로 4개의 선형 레이어를 쌓았으며 레이어마다 활성 함수로 LeakyReLU를 넣어줬다. 입력값으로 이미지 크기인 28x28개의 변수를 받은 뒤 레이어의 크기가 28x28에서 1024로, 512로, 256으로 점차 줄어들다. 마지막에는 확률값을 나타내는 숫자 하나를 출력한다.\n",
        "\n",
        "레이어마다 들어간 드롭아웃(Dropout)은 학습 시에 무작위로 절반의 뉴런을 사용하지 않도록 한다. 이를 통해 모델이 과적합(Overfitting, 오버피팅)되는 것을 방지할 수 있고, 또한 구분자가 생성자보다 지나치게 빨리 학습되는 것도 막을 수 있다. 구분자의 마지막 레이어에서는 출력값을 0과 1 사이로 만들기 위해 활성 함수로 Sigmoid를 넣었다.\n",
        "\n",
        "## <코드3> GAN의 구분자(Discriminator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    # 네트워크 구조\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=28 * 28, out_features=1024),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "            nn.Dropout(inplace=False),\n",
        "            nn.Linear(in_features=1024, out_features=512),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "            nn.Dropout(inplace=False),\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "            nn.Dropout(inplace=False),\n",
        "            nn.Linear(in_features=256, out_features=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아 이미지가 진짜일 확률을 0 ~ 1 사이로 출력한다.\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.view(-1, 28 * 28)\n",
        "        return self.main(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <코드4> 생성자와 구분자 객체 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = Generator()\n",
        "D = Discriminator()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제부터는 이렇게 만들어진 네트워크 구조를 학습하는 방법에 대해 알아보자. 학습하기 위해서는 모델을 평가할 수 있어야 한다. 모델의 평가 지표가 좋아지는 방향으로 매개 변수를 업데이트할 것이기 때문이다. 구분자의 출력값은 이미지가 진짜일 확률이고, 이 확률이 얼마나 정답과 가까운지를 측정하기 위해 바이너리 크로스 엔트로피(Binary cross entropy) 손실 함수(loss function)를 사용한다. 이 함수는 구분자가 출력한 확률값이 정답에 가까우면 낮아지고 정답에서 멀면 높아진다. 이 손실 함수의 값을 낮추는 것이 모델 학습의 목표가 된다.\n",
        "\n",
        "이제 생성자와 구분자의 매개 변수를 업데이트하는 최적화 함수가 각각 하나씩 필요하다. 최적화 기법에는 여러 종류가 있지만 여기서는 가장 널리 쓰이는 기법인 아담(Adam)을 사용했다. 아담은 매개 변수마다 업데이트 속도를 최적으로 조절하는 효율적인 최적화 기법이다.\n",
        "\n",
        "## <코드5> 손실 함수와 최적화 기법 지정하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary Cross Entropy loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 생성자의 매개 변수를 최적화하는 Adam optimizer\n",
        "G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# 구분자의 매개 변수를 최적화하는 Adam optimizer\n",
        "D_optimier = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 학습을 위해서 전체 데이터셋을 여러 번 돌며 매개 변수를 조금씩 업데이트한다. 데이터셋을 한 번 도는 것을 1 에폭(Epoch)이라고 부르는데, 여기서는 100 에폭 동안 학습할 것이다. 각 에폭마다 배치 사이즈(Batch Size)인 60개만큼 데이터를 가져와서 모델을 학습시킨다. MNIST 학습 데이터의 개수가 6만개이니 1에폭마다 1000번씩 학습이 이루어지는 셈이다.\n",
        "\n",
        "## <코드6> 모델 학습을 위한 반복문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데아터셋을 100번 돌며 학습한다.\n",
        "\n",
        "for epoch in range(100):\n",
        "    # 한 번에 batch_size만큼 데이터를 가져온다.\n",
        "    for real_data, _ in dataloader:\n",
        "        batch_size = real_data.size(0)\n",
        "    \n",
        "    # 데이터를 파이토치의 변수로 변환한다.\n",
        "        real_data = Variable(real_data)\n",
        "        # ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "먼저 구분자를 학습시켜보자. 구분자는 진짜 이미지를 입력하면 1에 가까운 확률값을 출력하고, 가짜 데이터를 입력하면 0에 가까운 확률값을 출력해야 한다. 따라서 구분자의 손실 함수는 두 가지의 합으로 이루어진다. 진짜 이미지를 입력했을 때의 출력값과 1과의 차이, 그리고 가짜 이미지를 입력했을 때의 출력값과 0과의 차이, 두 경우의 합이 구분자의 손실 함수다. 이 손실 함수의 값을 최소화하는 방향으로 구분자의 매개 변수가 업데이트된다.\n",
        "\n",
        "파이토치에서는 간단한 방법으로 역전파를 통해 계산된 각 변수의 미분 값을 구할 수 있다. 그 상태에서 최적화 함수를 실행시키면 매개 변수가 한번 업데이트된다.\n",
        "\n",
        "## <코드7> 구분자 학습시키기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 데아터셋을 100번 돌며 학습한다.\n",
        "\n",
        "for epoch in range(100):\n",
        "    # 한 번에 batch_size만큼 데이터를 가져온다.\n",
        "    for real_data, _ in dataloader:\n",
        "        batch_size = real_data.size(0)\n",
        "    \n",
        "    # 데이터를 파이토치의 변수로 변환한다.\n",
        "        real_data = Variable(real_data)\n",
        "\n",
        "        # ### ### ### 구분자 학습시키기 ### ### ### #\n",
        "        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n",
        "        # 정답지에 해당하는 변수를 만든다.\n",
        "\n",
        "        target_real = Variable(torch.ones(batch_size, 1))\n",
        "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
        "\n",
        "        # 진짜 이미지를 구분자에 넣는다.\n",
        "        D_result_from_real = D(real_data)\n",
        "\n",
        "        # 구분자의 출력 값이 정답지인 1에서 멀수록 Loss가 높아진다.\n",
        "        D_loss_real = criterion(D_result_from_real, target_real)\n",
        "\n",
        "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
        "        z = Variable(torch.randn((batch_size, 100)))\n",
        "\n",
        "        # 생성자로 가짜 이미지를 생성한다.\n",
        "        fake_data = G(z)\n",
        "\n",
        "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
        "        D_result_from_fake = D(fake_data)\n",
        "\n",
        "        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n",
        "        D_loss_fake = criterion(D_loss_fake, target_fake)\n",
        "\n",
        "        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n",
        "        D.zero_grad()\n",
        "\n",
        "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
        "        D_loss.backward()\n",
        "\n",
        "        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n",
        "        D_optimier.step()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음으로 생성자를 학습할 차례다. 생성자의 목적은 구분자를 속이는 것이다. 다시 말해 생성자가 만들어낸 가짜 이미지를 구분자에 넣었을 때 출력값이 1에 가깝게 나오도록 해야 한다. 이 값이 1에서 떨어진 정도가 생성자의 손실 함수가 되고, 이를 최소화 시키도록 생성자를 학습시키게 된다.\n",
        "\n",
        "## <코드8> 생성자 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Batch: 0, D Loss: 0.3382036089897156, G Loss: 2.695267677307129\n",
            "Epoch: 0, Batch: 100, D Loss: 0.5041618347167969, G Loss: 3.301342725753784\n",
            "Epoch: 0, Batch: 200, D Loss: 0.6896781921386719, G Loss: 2.6689257621765137\n",
            "Epoch: 0, Batch: 300, D Loss: 0.4603595435619354, G Loss: 1.5370315313339233\n",
            "Epoch: 0, Batch: 400, D Loss: 0.67864590883255, G Loss: 2.502753734588623\n",
            "Epoch: 0, Batch: 500, D Loss: 0.5591709613800049, G Loss: 1.5592728853225708\n",
            "Epoch: 0, Batch: 600, D Loss: 0.3569903075695038, G Loss: 2.7076470851898193\n",
            "Epoch: 0, Batch: 700, D Loss: 0.4840846657752991, G Loss: 2.5027434825897217\n",
            "Epoch: 0, Batch: 800, D Loss: 0.5758324861526489, G Loss: 1.991543173789978\n",
            "Epoch: 0, Batch: 900, D Loss: 0.6301521062850952, G Loss: 3.2881109714508057\n",
            "Epoch: 1, Batch: 0, D Loss: 0.5475175380706787, G Loss: 2.8206799030303955\n",
            "Epoch: 1, Batch: 100, D Loss: 0.8374646306037903, G Loss: 2.2299935817718506\n",
            "Epoch: 1, Batch: 200, D Loss: 0.512529194355011, G Loss: 1.6790850162506104\n",
            "Epoch: 1, Batch: 300, D Loss: 0.7269440293312073, G Loss: 3.3182058334350586\n",
            "Epoch: 1, Batch: 400, D Loss: 0.633474588394165, G Loss: 1.8928133249282837\n",
            "Epoch: 1, Batch: 500, D Loss: 0.5540033578872681, G Loss: 3.0141983032226562\n",
            "Epoch: 1, Batch: 600, D Loss: 0.8982604742050171, G Loss: 1.694834589958191\n",
            "Epoch: 1, Batch: 700, D Loss: 0.5943235754966736, G Loss: 1.9444941282272339\n",
            "Epoch: 1, Batch: 800, D Loss: 0.9282571077346802, G Loss: 1.4816009998321533\n",
            "Epoch: 1, Batch: 900, D Loss: 0.681847095489502, G Loss: 1.7477940320968628\n",
            "Epoch: 2, Batch: 0, D Loss: 0.7151879072189331, G Loss: 2.968040943145752\n",
            "Epoch: 2, Batch: 100, D Loss: 0.6808760166168213, G Loss: 1.786313533782959\n",
            "Epoch: 2, Batch: 200, D Loss: 0.7790871858596802, G Loss: 2.2174575328826904\n",
            "Epoch: 2, Batch: 300, D Loss: 0.7782541513442993, G Loss: 1.1994051933288574\n",
            "Epoch: 2, Batch: 400, D Loss: 0.7198986411094666, G Loss: 1.5759596824645996\n",
            "Epoch: 2, Batch: 500, D Loss: 0.7056560516357422, G Loss: 1.8777413368225098\n",
            "Epoch: 2, Batch: 600, D Loss: 0.9082581996917725, G Loss: 1.2160282135009766\n",
            "Epoch: 2, Batch: 700, D Loss: 0.8695735931396484, G Loss: 1.3342258930206299\n",
            "Epoch: 2, Batch: 800, D Loss: 0.821406900882721, G Loss: 1.3294597864151\n",
            "Epoch: 2, Batch: 900, D Loss: 0.7780237197875977, G Loss: 1.4920510053634644\n",
            "Epoch: 3, Batch: 0, D Loss: 0.7622675895690918, G Loss: 1.3585675954818726\n",
            "Epoch: 3, Batch: 100, D Loss: 0.935280442237854, G Loss: 1.3934364318847656\n",
            "Epoch: 3, Batch: 200, D Loss: 0.9169235825538635, G Loss: 2.023198366165161\n",
            "Epoch: 3, Batch: 300, D Loss: 0.8809579014778137, G Loss: 1.6418050527572632\n",
            "Epoch: 3, Batch: 400, D Loss: 0.8583157062530518, G Loss: 1.5828419923782349\n",
            "Epoch: 3, Batch: 500, D Loss: 0.7388511896133423, G Loss: 1.1254158020019531\n",
            "Epoch: 3, Batch: 600, D Loss: 0.839918315410614, G Loss: 1.6354339122772217\n",
            "Epoch: 3, Batch: 700, D Loss: 1.0486952066421509, G Loss: 1.6425689458847046\n",
            "Epoch: 3, Batch: 800, D Loss: 0.9884969592094421, G Loss: 1.8717108964920044\n",
            "Epoch: 3, Batch: 900, D Loss: 0.9771599769592285, G Loss: 1.3295875787734985\n",
            "Epoch: 4, Batch: 0, D Loss: 0.8109089136123657, G Loss: 1.4171465635299683\n",
            "Epoch: 4, Batch: 100, D Loss: 1.0404763221740723, G Loss: 1.5813286304473877\n",
            "Epoch: 4, Batch: 200, D Loss: 0.782097578048706, G Loss: 1.6310608386993408\n",
            "Epoch: 4, Batch: 300, D Loss: 1.1332201957702637, G Loss: 1.5165108442306519\n",
            "Epoch: 4, Batch: 400, D Loss: 0.7987446784973145, G Loss: 1.2113665342330933\n",
            "Epoch: 4, Batch: 500, D Loss: 0.8770294189453125, G Loss: 1.4646011590957642\n",
            "Epoch: 4, Batch: 600, D Loss: 1.1770946979522705, G Loss: 1.5904173851013184\n",
            "Epoch: 4, Batch: 700, D Loss: 1.0528138875961304, G Loss: 1.289126992225647\n",
            "Epoch: 4, Batch: 800, D Loss: 1.0645713806152344, G Loss: 1.6354564428329468\n",
            "Epoch: 4, Batch: 900, D Loss: 0.9156908988952637, G Loss: 1.5306130647659302\n",
            "Epoch: 5, Batch: 0, D Loss: 0.950514554977417, G Loss: 1.6781539916992188\n",
            "Epoch: 5, Batch: 100, D Loss: 0.703658938407898, G Loss: 1.3506319522857666\n",
            "Epoch: 5, Batch: 200, D Loss: 0.9733030796051025, G Loss: 1.1384961605072021\n",
            "Epoch: 5, Batch: 300, D Loss: 1.0194615125656128, G Loss: 1.0478004217147827\n",
            "Epoch: 5, Batch: 400, D Loss: 1.0076661109924316, G Loss: 1.4321386814117432\n",
            "Epoch: 5, Batch: 500, D Loss: 0.9932563304901123, G Loss: 1.1760739088058472\n",
            "Epoch: 5, Batch: 600, D Loss: 1.118417501449585, G Loss: 1.3472203016281128\n",
            "Epoch: 5, Batch: 700, D Loss: 0.9365352392196655, G Loss: 1.1436716318130493\n",
            "Epoch: 5, Batch: 800, D Loss: 0.901412844657898, G Loss: 1.6740107536315918\n",
            "Epoch: 5, Batch: 900, D Loss: 1.160487413406372, G Loss: 1.419365644454956\n",
            "Epoch: 6, Batch: 0, D Loss: 0.9415709972381592, G Loss: 1.311964511871338\n",
            "Epoch: 6, Batch: 100, D Loss: 1.0427608489990234, G Loss: 1.1413633823394775\n",
            "Epoch: 6, Batch: 200, D Loss: 1.0613048076629639, G Loss: 1.1231026649475098\n",
            "Epoch: 6, Batch: 300, D Loss: 1.009366750717163, G Loss: 1.0550590753555298\n",
            "Epoch: 6, Batch: 400, D Loss: 1.2977778911590576, G Loss: 1.385798692703247\n",
            "Epoch: 6, Batch: 500, D Loss: 1.1207122802734375, G Loss: 1.099030613899231\n",
            "Epoch: 6, Batch: 600, D Loss: 1.0024852752685547, G Loss: 1.2086522579193115\n",
            "Epoch: 6, Batch: 700, D Loss: 1.1101512908935547, G Loss: 1.093448519706726\n",
            "Epoch: 6, Batch: 800, D Loss: 1.131170630455017, G Loss: 1.2975528240203857\n",
            "Epoch: 6, Batch: 900, D Loss: 1.011188268661499, G Loss: 1.1395621299743652\n",
            "Epoch: 7, Batch: 0, D Loss: 1.2087159156799316, G Loss: 1.8444478511810303\n",
            "Epoch: 7, Batch: 100, D Loss: 1.2748901844024658, G Loss: 1.2960631847381592\n",
            "Epoch: 7, Batch: 200, D Loss: 1.0056774616241455, G Loss: 0.8113638758659363\n",
            "Epoch: 7, Batch: 300, D Loss: 1.0985379219055176, G Loss: 1.0611300468444824\n",
            "Epoch: 7, Batch: 400, D Loss: 1.2174428701400757, G Loss: 0.8373333215713501\n",
            "Epoch: 7, Batch: 500, D Loss: 1.1325082778930664, G Loss: 1.150506854057312\n",
            "Epoch: 7, Batch: 600, D Loss: 1.0580545663833618, G Loss: 1.1082730293273926\n",
            "Epoch: 7, Batch: 700, D Loss: 1.0616698265075684, G Loss: 1.1475260257720947\n",
            "Epoch: 7, Batch: 800, D Loss: 1.0854988098144531, G Loss: 1.0194530487060547\n",
            "Epoch: 7, Batch: 900, D Loss: 1.090139627456665, G Loss: 0.8308926224708557\n",
            "Epoch: 8, Batch: 0, D Loss: 1.2122300863265991, G Loss: 1.0412119626998901\n",
            "Epoch: 8, Batch: 100, D Loss: 1.1776316165924072, G Loss: 1.1116565465927124\n",
            "Epoch: 8, Batch: 200, D Loss: 1.094731092453003, G Loss: 1.0667729377746582\n",
            "Epoch: 8, Batch: 300, D Loss: 1.1335256099700928, G Loss: 1.1100558042526245\n",
            "Epoch: 8, Batch: 400, D Loss: 1.1010843515396118, G Loss: 1.0539494752883911\n",
            "Epoch: 8, Batch: 500, D Loss: 1.2340965270996094, G Loss: 0.955435574054718\n",
            "Epoch: 8, Batch: 600, D Loss: 0.9782582521438599, G Loss: 1.0928871631622314\n",
            "Epoch: 8, Batch: 700, D Loss: 1.12717604637146, G Loss: 1.0097395181655884\n",
            "Epoch: 8, Batch: 800, D Loss: 1.282845377922058, G Loss: 0.9451216459274292\n",
            "Epoch: 8, Batch: 900, D Loss: 1.1876329183578491, G Loss: 1.094421148300171\n",
            "Epoch: 9, Batch: 0, D Loss: 1.095344066619873, G Loss: 1.1009358167648315\n",
            "Epoch: 9, Batch: 100, D Loss: 1.2157349586486816, G Loss: 0.9952396750450134\n",
            "Epoch: 9, Batch: 200, D Loss: 1.1693099737167358, G Loss: 1.1161248683929443\n",
            "Epoch: 9, Batch: 300, D Loss: 1.263451099395752, G Loss: 0.9900282621383667\n",
            "Epoch: 9, Batch: 400, D Loss: 1.0276744365692139, G Loss: 0.8794897794723511\n",
            "Epoch: 9, Batch: 500, D Loss: 0.9857368469238281, G Loss: 1.1209208965301514\n",
            "Epoch: 9, Batch: 600, D Loss: 1.1636879444122314, G Loss: 1.20905339717865\n",
            "Epoch: 9, Batch: 700, D Loss: 1.2149150371551514, G Loss: 0.975679337978363\n",
            "Epoch: 9, Batch: 800, D Loss: 1.2229620218276978, G Loss: 0.9917894005775452\n",
            "Epoch: 9, Batch: 900, D Loss: 1.2218759059906006, G Loss: 1.155465006828308\n",
            "Epoch: 10, Batch: 0, D Loss: 1.239696979522705, G Loss: 0.9642924070358276\n",
            "Epoch: 10, Batch: 100, D Loss: 1.1650784015655518, G Loss: 1.0400062799453735\n",
            "Epoch: 10, Batch: 200, D Loss: 1.1921778917312622, G Loss: 1.0582982301712036\n",
            "Epoch: 10, Batch: 300, D Loss: 1.085402250289917, G Loss: 1.0790072679519653\n",
            "Epoch: 10, Batch: 400, D Loss: 1.0701971054077148, G Loss: 1.059303641319275\n",
            "Epoch: 10, Batch: 500, D Loss: 1.1241806745529175, G Loss: 0.7593375444412231\n",
            "Epoch: 10, Batch: 600, D Loss: 1.0739580392837524, G Loss: 0.9077972173690796\n",
            "Epoch: 10, Batch: 700, D Loss: 1.0647705793380737, G Loss: 1.059450626373291\n",
            "Epoch: 10, Batch: 800, D Loss: 1.170172929763794, G Loss: 0.9344770312309265\n",
            "Epoch: 10, Batch: 900, D Loss: 1.382157802581787, G Loss: 0.9277711510658264\n",
            "Epoch: 11, Batch: 0, D Loss: 1.1430896520614624, G Loss: 1.0824129581451416\n",
            "Epoch: 11, Batch: 100, D Loss: 1.2335095405578613, G Loss: 0.9311978220939636\n",
            "Epoch: 11, Batch: 200, D Loss: 1.1703609228134155, G Loss: 1.1946488618850708\n",
            "Epoch: 11, Batch: 300, D Loss: 1.1888946294784546, G Loss: 1.0814290046691895\n",
            "Epoch: 11, Batch: 400, D Loss: 1.2185986042022705, G Loss: 0.9345998764038086\n",
            "Epoch: 11, Batch: 500, D Loss: 1.1787713766098022, G Loss: 0.9956327676773071\n",
            "Epoch: 11, Batch: 600, D Loss: 1.244880199432373, G Loss: 1.060352087020874\n",
            "Epoch: 11, Batch: 700, D Loss: 1.1232218742370605, G Loss: 1.1512794494628906\n",
            "Epoch: 11, Batch: 800, D Loss: 1.1253348588943481, G Loss: 0.8879528045654297\n",
            "Epoch: 11, Batch: 900, D Loss: 1.258098840713501, G Loss: 1.0182850360870361\n",
            "Epoch: 12, Batch: 0, D Loss: 1.3049936294555664, G Loss: 1.087127685546875\n",
            "Epoch: 12, Batch: 100, D Loss: 1.1293010711669922, G Loss: 1.0174311399459839\n",
            "Epoch: 12, Batch: 200, D Loss: 1.1884055137634277, G Loss: 1.073962926864624\n",
            "Epoch: 12, Batch: 300, D Loss: 1.0626870393753052, G Loss: 1.0554436445236206\n",
            "Epoch: 12, Batch: 400, D Loss: 1.3371851444244385, G Loss: 0.9945952296257019\n",
            "Epoch: 12, Batch: 500, D Loss: 1.219185709953308, G Loss: 0.9611419439315796\n",
            "Epoch: 12, Batch: 600, D Loss: 1.1662836074829102, G Loss: 1.0389196872711182\n",
            "Epoch: 12, Batch: 700, D Loss: 1.166164517402649, G Loss: 0.9909632802009583\n",
            "Epoch: 12, Batch: 800, D Loss: 1.1638582944869995, G Loss: 1.0648884773254395\n",
            "Epoch: 12, Batch: 900, D Loss: 1.0743321180343628, G Loss: 0.7689230442047119\n",
            "Epoch: 13, Batch: 0, D Loss: 1.178161382675171, G Loss: 0.8878282904624939\n",
            "Epoch: 13, Batch: 100, D Loss: 1.1918385028839111, G Loss: 1.026129126548767\n",
            "Epoch: 13, Batch: 200, D Loss: 1.1677634716033936, G Loss: 0.8618654012680054\n",
            "Epoch: 13, Batch: 300, D Loss: 1.352173089981079, G Loss: 0.9495859742164612\n",
            "Epoch: 13, Batch: 400, D Loss: 1.2464988231658936, G Loss: 1.009130597114563\n",
            "Epoch: 13, Batch: 500, D Loss: 1.2278329133987427, G Loss: 0.7799437642097473\n",
            "Epoch: 13, Batch: 600, D Loss: 1.2066243886947632, G Loss: 0.8641249537467957\n",
            "Epoch: 13, Batch: 700, D Loss: 1.1053764820098877, G Loss: 0.9867303371429443\n",
            "Epoch: 13, Batch: 800, D Loss: 1.1945476531982422, G Loss: 0.9569244980812073\n",
            "Epoch: 13, Batch: 900, D Loss: 1.149484634399414, G Loss: 0.8191797137260437\n",
            "Epoch: 14, Batch: 0, D Loss: 1.3034861087799072, G Loss: 1.0606834888458252\n",
            "Epoch: 14, Batch: 100, D Loss: 1.2731763124465942, G Loss: 0.8959376811981201\n",
            "Epoch: 14, Batch: 200, D Loss: 1.3188979625701904, G Loss: 1.0432850122451782\n",
            "Epoch: 14, Batch: 300, D Loss: 1.2715336084365845, G Loss: 1.0481542348861694\n",
            "Epoch: 14, Batch: 400, D Loss: 1.2238640785217285, G Loss: 0.9198675751686096\n",
            "Epoch: 14, Batch: 500, D Loss: 1.253901720046997, G Loss: 1.051202654838562\n",
            "Epoch: 14, Batch: 600, D Loss: 1.129378318786621, G Loss: 0.9845901727676392\n",
            "Epoch: 14, Batch: 700, D Loss: 1.2523720264434814, G Loss: 1.0096073150634766\n",
            "Epoch: 14, Batch: 800, D Loss: 1.1675562858581543, G Loss: 0.8912634253501892\n",
            "Epoch: 14, Batch: 900, D Loss: 1.2495293617248535, G Loss: 0.9251986145973206\n",
            "Epoch: 15, Batch: 0, D Loss: 1.2789239883422852, G Loss: 0.8199861645698547\n",
            "Epoch: 15, Batch: 100, D Loss: 1.0925230979919434, G Loss: 0.9395589232444763\n",
            "Epoch: 15, Batch: 200, D Loss: 1.2339109182357788, G Loss: 0.8527118563652039\n",
            "Epoch: 15, Batch: 300, D Loss: 1.3263916969299316, G Loss: 0.9270147085189819\n",
            "Epoch: 15, Batch: 400, D Loss: 1.2797236442565918, G Loss: 0.9095144271850586\n",
            "Epoch: 15, Batch: 500, D Loss: 1.1879632472991943, G Loss: 0.9596064686775208\n",
            "Epoch: 15, Batch: 600, D Loss: 1.1562538146972656, G Loss: 0.9729277491569519\n",
            "Epoch: 15, Batch: 700, D Loss: 1.2625315189361572, G Loss: 1.0701881647109985\n",
            "Epoch: 15, Batch: 800, D Loss: 1.294335126876831, G Loss: 0.9656801223754883\n",
            "Epoch: 15, Batch: 900, D Loss: 1.284027338027954, G Loss: 1.0231002569198608\n",
            "Epoch: 16, Batch: 0, D Loss: 1.2527451515197754, G Loss: 0.9246771931648254\n",
            "Epoch: 16, Batch: 100, D Loss: 1.3246583938598633, G Loss: 0.9286842942237854\n",
            "Epoch: 16, Batch: 200, D Loss: 1.198598861694336, G Loss: 0.9480177164077759\n",
            "Epoch: 16, Batch: 300, D Loss: 1.195040225982666, G Loss: 0.8910263180732727\n",
            "Epoch: 16, Batch: 400, D Loss: 1.3114672899246216, G Loss: 1.0773428678512573\n",
            "Epoch: 16, Batch: 500, D Loss: 1.1664732694625854, G Loss: 0.8930886387825012\n",
            "Epoch: 16, Batch: 600, D Loss: 1.2667633295059204, G Loss: 0.9396148324012756\n",
            "Epoch: 16, Batch: 700, D Loss: 1.1423630714416504, G Loss: 0.9694679379463196\n",
            "Epoch: 16, Batch: 800, D Loss: 1.2139853239059448, G Loss: 0.7572118639945984\n",
            "Epoch: 16, Batch: 900, D Loss: 1.2632040977478027, G Loss: 0.8329779505729675\n",
            "Epoch: 17, Batch: 0, D Loss: 1.1256632804870605, G Loss: 0.833865225315094\n",
            "Epoch: 17, Batch: 100, D Loss: 1.2383795976638794, G Loss: 0.8656789064407349\n",
            "Epoch: 17, Batch: 200, D Loss: 1.330470323562622, G Loss: 0.8800415396690369\n",
            "Epoch: 17, Batch: 300, D Loss: 1.3606257438659668, G Loss: 0.9353510737419128\n",
            "Epoch: 17, Batch: 400, D Loss: 1.165008306503296, G Loss: 0.9080230593681335\n",
            "Epoch: 17, Batch: 500, D Loss: 1.311964511871338, G Loss: 1.0183559656143188\n",
            "Epoch: 17, Batch: 600, D Loss: 1.2680107355117798, G Loss: 0.937994658946991\n",
            "Epoch: 17, Batch: 700, D Loss: 1.1587945222854614, G Loss: 0.8111631870269775\n",
            "Epoch: 17, Batch: 800, D Loss: 1.2330574989318848, G Loss: 0.853976309299469\n",
            "Epoch: 17, Batch: 900, D Loss: 1.2328561544418335, G Loss: 0.9192407131195068\n",
            "Epoch: 18, Batch: 0, D Loss: 1.3507373332977295, G Loss: 0.9728334546089172\n",
            "Epoch: 18, Batch: 100, D Loss: 1.1669583320617676, G Loss: 1.0231235027313232\n",
            "Epoch: 18, Batch: 200, D Loss: 1.3485093116760254, G Loss: 0.8899324536323547\n",
            "Epoch: 18, Batch: 300, D Loss: 1.2442007064819336, G Loss: 0.8458929061889648\n",
            "Epoch: 18, Batch: 400, D Loss: 1.3660318851470947, G Loss: 0.7791747450828552\n",
            "Epoch: 18, Batch: 500, D Loss: 1.2867584228515625, G Loss: 0.9421546459197998\n",
            "Epoch: 18, Batch: 600, D Loss: 1.4396841526031494, G Loss: 0.7922612428665161\n",
            "Epoch: 18, Batch: 700, D Loss: 1.2791707515716553, G Loss: 0.797649621963501\n",
            "Epoch: 18, Batch: 800, D Loss: 1.2123202085494995, G Loss: 0.8440370559692383\n",
            "Epoch: 18, Batch: 900, D Loss: 1.1515779495239258, G Loss: 0.8021100163459778\n",
            "Epoch: 19, Batch: 0, D Loss: 1.2856082916259766, G Loss: 0.8959811925888062\n",
            "Epoch: 19, Batch: 100, D Loss: 1.3157885074615479, G Loss: 0.8849067687988281\n",
            "Epoch: 19, Batch: 200, D Loss: 1.3142225742340088, G Loss: 1.0180879831314087\n",
            "Epoch: 19, Batch: 300, D Loss: 1.2384018898010254, G Loss: 0.9433353543281555\n",
            "Epoch: 19, Batch: 400, D Loss: 1.2523409128189087, G Loss: 0.9379689693450928\n",
            "Epoch: 19, Batch: 500, D Loss: 1.3719021081924438, G Loss: 0.9656606316566467\n",
            "Epoch: 19, Batch: 600, D Loss: 1.3864827156066895, G Loss: 0.7148628234863281\n",
            "Epoch: 19, Batch: 700, D Loss: 1.2867944240570068, G Loss: 0.8848786950111389\n",
            "Epoch: 19, Batch: 800, D Loss: 1.1942592859268188, G Loss: 0.8737356662750244\n",
            "Epoch: 19, Batch: 900, D Loss: 1.2084476947784424, G Loss: 1.0444560050964355\n",
            "Epoch: 20, Batch: 0, D Loss: 1.2516555786132812, G Loss: 1.0526065826416016\n",
            "Epoch: 20, Batch: 100, D Loss: 1.275089144706726, G Loss: 0.938269317150116\n",
            "Epoch: 20, Batch: 200, D Loss: 1.227914571762085, G Loss: 0.9559995532035828\n",
            "Epoch: 20, Batch: 300, D Loss: 1.183892846107483, G Loss: 0.9992966055870056\n",
            "Epoch: 20, Batch: 400, D Loss: 1.277376651763916, G Loss: 0.9197432994842529\n",
            "Epoch: 20, Batch: 500, D Loss: 1.2339856624603271, G Loss: 0.8126972913742065\n",
            "Epoch: 20, Batch: 600, D Loss: 1.2663131952285767, G Loss: 0.8761529922485352\n",
            "Epoch: 20, Batch: 700, D Loss: 1.276503086090088, G Loss: 0.9597628116607666\n",
            "Epoch: 20, Batch: 800, D Loss: 1.2312445640563965, G Loss: 0.988871693611145\n",
            "Epoch: 20, Batch: 900, D Loss: 1.3152472972869873, G Loss: 0.9894047379493713\n",
            "Epoch: 21, Batch: 0, D Loss: 1.3186616897583008, G Loss: 1.1021802425384521\n",
            "Epoch: 21, Batch: 100, D Loss: 1.233398675918579, G Loss: 0.895800769329071\n",
            "Epoch: 21, Batch: 200, D Loss: 1.1908104419708252, G Loss: 1.032387375831604\n",
            "Epoch: 21, Batch: 300, D Loss: 1.1604642868041992, G Loss: 0.9958038330078125\n",
            "Epoch: 21, Batch: 400, D Loss: 1.3269360065460205, G Loss: 0.9373772740364075\n",
            "Epoch: 21, Batch: 500, D Loss: 1.228296160697937, G Loss: 0.8359543681144714\n",
            "Epoch: 21, Batch: 600, D Loss: 1.3181499242782593, G Loss: 0.8488600850105286\n",
            "Epoch: 21, Batch: 700, D Loss: 1.3364951610565186, G Loss: 0.9324458837509155\n",
            "Epoch: 21, Batch: 800, D Loss: 1.2768498659133911, G Loss: 0.8677749633789062\n",
            "Epoch: 21, Batch: 900, D Loss: 1.273075819015503, G Loss: 0.9242600798606873\n",
            "Epoch: 22, Batch: 0, D Loss: 1.230374813079834, G Loss: 0.8505972027778625\n",
            "Epoch: 22, Batch: 100, D Loss: 1.2431617975234985, G Loss: 0.8389677405357361\n",
            "Epoch: 22, Batch: 200, D Loss: 1.2912921905517578, G Loss: 0.8439797163009644\n",
            "Epoch: 22, Batch: 300, D Loss: 1.2806583642959595, G Loss: 0.7874307632446289\n",
            "Epoch: 22, Batch: 400, D Loss: 1.2361831665039062, G Loss: 0.7517256140708923\n",
            "Epoch: 22, Batch: 500, D Loss: 1.2333343029022217, G Loss: 1.038804292678833\n",
            "Epoch: 22, Batch: 600, D Loss: 1.2176058292388916, G Loss: 0.9780544638633728\n",
            "Epoch: 22, Batch: 700, D Loss: 1.2207019329071045, G Loss: 0.8456153273582458\n",
            "Epoch: 22, Batch: 800, D Loss: 1.2760341167449951, G Loss: 0.8426628112792969\n",
            "Epoch: 22, Batch: 900, D Loss: 1.2494679689407349, G Loss: 0.8407200574874878\n",
            "Epoch: 23, Batch: 0, D Loss: 1.2949650287628174, G Loss: 0.9581846594810486\n",
            "Epoch: 23, Batch: 100, D Loss: 1.2316848039627075, G Loss: 0.7340549230575562\n",
            "Epoch: 23, Batch: 200, D Loss: 1.3165240287780762, G Loss: 0.906217634677887\n",
            "Epoch: 23, Batch: 300, D Loss: 1.2756624221801758, G Loss: 0.8702195286750793\n",
            "Epoch: 23, Batch: 400, D Loss: 1.2495907545089722, G Loss: 0.9581155180931091\n",
            "Epoch: 23, Batch: 500, D Loss: 1.2708501815795898, G Loss: 0.8489322662353516\n",
            "Epoch: 23, Batch: 600, D Loss: 1.2679593563079834, G Loss: 0.7380692958831787\n",
            "Epoch: 23, Batch: 700, D Loss: 1.3218045234680176, G Loss: 0.8774416446685791\n",
            "Epoch: 23, Batch: 800, D Loss: 1.3519134521484375, G Loss: 0.8777230381965637\n",
            "Epoch: 23, Batch: 900, D Loss: 1.2738794088363647, G Loss: 0.917361319065094\n",
            "Epoch: 24, Batch: 0, D Loss: 1.3317606449127197, G Loss: 0.9243243932723999\n",
            "Epoch: 24, Batch: 100, D Loss: 1.2799360752105713, G Loss: 0.8828223943710327\n",
            "Epoch: 24, Batch: 200, D Loss: 1.226133108139038, G Loss: 0.9984376430511475\n",
            "Epoch: 24, Batch: 300, D Loss: 1.2793941497802734, G Loss: 0.857457160949707\n",
            "Epoch: 24, Batch: 400, D Loss: 1.2520207166671753, G Loss: 0.9406102299690247\n",
            "Epoch: 24, Batch: 500, D Loss: 1.2560362815856934, G Loss: 0.9808821082115173\n",
            "Epoch: 24, Batch: 600, D Loss: 1.3074958324432373, G Loss: 0.8382776975631714\n",
            "Epoch: 24, Batch: 700, D Loss: 1.2434577941894531, G Loss: 0.7908269762992859\n",
            "Epoch: 24, Batch: 800, D Loss: 1.2341248989105225, G Loss: 0.9405660033226013\n",
            "Epoch: 24, Batch: 900, D Loss: 1.2529716491699219, G Loss: 0.91800856590271\n",
            "Epoch: 25, Batch: 0, D Loss: 1.2662689685821533, G Loss: 0.980417788028717\n",
            "Epoch: 25, Batch: 100, D Loss: 1.344861388206482, G Loss: 0.8520172238349915\n",
            "Epoch: 25, Batch: 200, D Loss: 1.3371739387512207, G Loss: 0.7877845764160156\n",
            "Epoch: 25, Batch: 300, D Loss: 1.2783129215240479, G Loss: 0.900191605091095\n",
            "Epoch: 25, Batch: 400, D Loss: 1.2502765655517578, G Loss: 0.841080904006958\n",
            "Epoch: 25, Batch: 500, D Loss: 1.3729910850524902, G Loss: 0.8882907629013062\n",
            "Epoch: 25, Batch: 600, D Loss: 1.304202914237976, G Loss: 0.8003070950508118\n",
            "Epoch: 25, Batch: 700, D Loss: 1.3887641429901123, G Loss: 0.904653787612915\n",
            "Epoch: 25, Batch: 800, D Loss: 1.3193447589874268, G Loss: 0.9875555038452148\n",
            "Epoch: 25, Batch: 900, D Loss: 1.349111795425415, G Loss: 0.956788957118988\n",
            "Epoch: 26, Batch: 0, D Loss: 1.2284520864486694, G Loss: 0.829882800579071\n",
            "Epoch: 26, Batch: 100, D Loss: 1.2547391653060913, G Loss: 1.0307186841964722\n",
            "Epoch: 26, Batch: 200, D Loss: 1.229417085647583, G Loss: 0.8164922595024109\n",
            "Epoch: 26, Batch: 300, D Loss: 1.2060840129852295, G Loss: 0.9875361323356628\n",
            "Epoch: 26, Batch: 400, D Loss: 1.2832322120666504, G Loss: 0.8170116543769836\n",
            "Epoch: 26, Batch: 500, D Loss: 1.2887189388275146, G Loss: 0.894763171672821\n",
            "Epoch: 26, Batch: 600, D Loss: 1.188215732574463, G Loss: 0.9171886444091797\n",
            "Epoch: 26, Batch: 700, D Loss: 1.2983722686767578, G Loss: 0.9980895519256592\n",
            "Epoch: 26, Batch: 800, D Loss: 1.2835112810134888, G Loss: 0.8743588924407959\n",
            "Epoch: 26, Batch: 900, D Loss: 1.3387219905853271, G Loss: 0.8730966448783875\n",
            "Epoch: 27, Batch: 0, D Loss: 1.1574018001556396, G Loss: 0.7950310111045837\n",
            "Epoch: 27, Batch: 100, D Loss: 1.197269320487976, G Loss: 0.8328776955604553\n",
            "Epoch: 27, Batch: 200, D Loss: 1.2868162393569946, G Loss: 0.8725273609161377\n",
            "Epoch: 27, Batch: 300, D Loss: 1.3206298351287842, G Loss: 0.7254968881607056\n",
            "Epoch: 27, Batch: 400, D Loss: 1.2469873428344727, G Loss: 0.9236646294593811\n",
            "Epoch: 27, Batch: 500, D Loss: 1.2196667194366455, G Loss: 0.8333339691162109\n",
            "Epoch: 27, Batch: 600, D Loss: 1.2926018238067627, G Loss: 0.957046389579773\n",
            "Epoch: 27, Batch: 700, D Loss: 1.2367557287216187, G Loss: 0.9788006544113159\n",
            "Epoch: 27, Batch: 800, D Loss: 1.2885608673095703, G Loss: 0.7655637264251709\n",
            "Epoch: 27, Batch: 900, D Loss: 1.263759732246399, G Loss: 0.8404393792152405\n",
            "Epoch: 28, Batch: 0, D Loss: 1.2559611797332764, G Loss: 0.8132213354110718\n",
            "Epoch: 28, Batch: 100, D Loss: 1.2611496448516846, G Loss: 0.8494450449943542\n",
            "Epoch: 28, Batch: 200, D Loss: 1.2419753074645996, G Loss: 0.9405994415283203\n",
            "Epoch: 28, Batch: 300, D Loss: 1.3416696786880493, G Loss: 0.9194104671478271\n",
            "Epoch: 28, Batch: 400, D Loss: 1.309356927871704, G Loss: 0.7737547755241394\n",
            "Epoch: 28, Batch: 500, D Loss: 1.2106835842132568, G Loss: 0.993655264377594\n",
            "Epoch: 28, Batch: 600, D Loss: 1.3557467460632324, G Loss: 0.8765022158622742\n",
            "Epoch: 28, Batch: 700, D Loss: 1.323129415512085, G Loss: 0.9397929906845093\n",
            "Epoch: 28, Batch: 800, D Loss: 1.277986764907837, G Loss: 0.8433240652084351\n",
            "Epoch: 28, Batch: 900, D Loss: 1.2870427370071411, G Loss: 0.8704500198364258\n",
            "Epoch: 29, Batch: 0, D Loss: 1.3351190090179443, G Loss: 0.928253173828125\n",
            "Epoch: 29, Batch: 100, D Loss: 1.2829768657684326, G Loss: 0.8267753720283508\n",
            "Epoch: 29, Batch: 200, D Loss: 1.2808693647384644, G Loss: 0.8217225670814514\n",
            "Epoch: 29, Batch: 300, D Loss: 1.2918375730514526, G Loss: 0.8831145167350769\n",
            "Epoch: 29, Batch: 400, D Loss: 1.2718143463134766, G Loss: 0.774544358253479\n",
            "Epoch: 29, Batch: 500, D Loss: 1.23650062084198, G Loss: 0.8034182786941528\n",
            "Epoch: 29, Batch: 600, D Loss: 1.250048279762268, G Loss: 0.8059336543083191\n",
            "Epoch: 29, Batch: 700, D Loss: 1.3349828720092773, G Loss: 0.8514721393585205\n",
            "Epoch: 29, Batch: 800, D Loss: 1.344369649887085, G Loss: 0.8256563544273376\n",
            "Epoch: 29, Batch: 900, D Loss: 1.2249683141708374, G Loss: 0.8071286082267761\n",
            "Epoch: 30, Batch: 0, D Loss: 1.2869822978973389, G Loss: 0.8147793412208557\n",
            "Epoch: 30, Batch: 100, D Loss: 1.3446694612503052, G Loss: 0.9734503626823425\n",
            "Epoch: 30, Batch: 200, D Loss: 1.2057063579559326, G Loss: 0.8234196305274963\n",
            "Epoch: 30, Batch: 300, D Loss: 1.2273738384246826, G Loss: 0.8928168416023254\n",
            "Epoch: 30, Batch: 400, D Loss: 1.297865629196167, G Loss: 0.7852875590324402\n",
            "Epoch: 30, Batch: 500, D Loss: 1.1864118576049805, G Loss: 0.8313104510307312\n",
            "Epoch: 30, Batch: 600, D Loss: 1.1630616188049316, G Loss: 0.8077987432479858\n",
            "Epoch: 30, Batch: 700, D Loss: 1.1378791332244873, G Loss: 0.7571085095405579\n",
            "Epoch: 30, Batch: 800, D Loss: 1.3128297328948975, G Loss: 0.9086943864822388\n",
            "Epoch: 30, Batch: 900, D Loss: 1.1855005025863647, G Loss: 0.9492431282997131\n",
            "Epoch: 31, Batch: 0, D Loss: 1.2866294384002686, G Loss: 0.9318207502365112\n",
            "Epoch: 31, Batch: 100, D Loss: 1.2892630100250244, G Loss: 0.887482762336731\n",
            "Epoch: 31, Batch: 200, D Loss: 1.3134222030639648, G Loss: 0.9669322967529297\n",
            "Epoch: 31, Batch: 300, D Loss: 1.2892944812774658, G Loss: 0.8778864741325378\n",
            "Epoch: 31, Batch: 400, D Loss: 1.2601618766784668, G Loss: 0.9471628069877625\n",
            "Epoch: 31, Batch: 500, D Loss: 1.146108627319336, G Loss: 1.00141441822052\n",
            "Epoch: 31, Batch: 600, D Loss: 1.361283302307129, G Loss: 0.8320661783218384\n",
            "Epoch: 31, Batch: 700, D Loss: 1.2214890718460083, G Loss: 0.831238329410553\n",
            "Epoch: 31, Batch: 800, D Loss: 1.332592248916626, G Loss: 0.8670969605445862\n",
            "Epoch: 31, Batch: 900, D Loss: 1.109800100326538, G Loss: 0.8285682201385498\n",
            "Epoch: 32, Batch: 0, D Loss: 1.3494607210159302, G Loss: 0.8599096536636353\n",
            "Epoch: 32, Batch: 100, D Loss: 1.2812292575836182, G Loss: 0.9026182889938354\n",
            "Epoch: 32, Batch: 200, D Loss: 1.289766550064087, G Loss: 0.9069766998291016\n",
            "Epoch: 32, Batch: 300, D Loss: 1.2571371793746948, G Loss: 0.9165747165679932\n",
            "Epoch: 32, Batch: 400, D Loss: 1.1992976665496826, G Loss: 0.9681053757667542\n",
            "Epoch: 32, Batch: 500, D Loss: 1.1770119667053223, G Loss: 0.7938317656517029\n",
            "Epoch: 32, Batch: 600, D Loss: 1.2036380767822266, G Loss: 0.8351300358772278\n",
            "Epoch: 32, Batch: 700, D Loss: 1.2756131887435913, G Loss: 0.7928562164306641\n",
            "Epoch: 32, Batch: 800, D Loss: 1.2814748287200928, G Loss: 0.8847624659538269\n",
            "Epoch: 32, Batch: 900, D Loss: 1.2450752258300781, G Loss: 0.8698570132255554\n",
            "Epoch: 33, Batch: 0, D Loss: 1.275801181793213, G Loss: 0.9947149157524109\n",
            "Epoch: 33, Batch: 100, D Loss: 1.342698574066162, G Loss: 0.920136570930481\n",
            "Epoch: 33, Batch: 200, D Loss: 1.3115086555480957, G Loss: 0.8442195653915405\n",
            "Epoch: 33, Batch: 300, D Loss: 1.2456656694412231, G Loss: 0.7674227356910706\n",
            "Epoch: 33, Batch: 400, D Loss: 1.312424659729004, G Loss: 0.9268978834152222\n",
            "Epoch: 33, Batch: 500, D Loss: 1.303389310836792, G Loss: 0.8123588562011719\n",
            "Epoch: 33, Batch: 600, D Loss: 1.2245783805847168, G Loss: 0.9225623607635498\n",
            "Epoch: 33, Batch: 700, D Loss: 1.282205581665039, G Loss: 0.8645827174186707\n",
            "Epoch: 33, Batch: 800, D Loss: 1.3113783597946167, G Loss: 0.8365048170089722\n",
            "Epoch: 33, Batch: 900, D Loss: 1.17751145362854, G Loss: 0.7410021424293518\n",
            "Epoch: 34, Batch: 0, D Loss: 1.3591010570526123, G Loss: 0.893159806728363\n",
            "Epoch: 34, Batch: 100, D Loss: 1.1986298561096191, G Loss: 0.7439395785331726\n",
            "Epoch: 34, Batch: 200, D Loss: 1.2697417736053467, G Loss: 0.9218313694000244\n",
            "Epoch: 34, Batch: 300, D Loss: 1.1981360912322998, G Loss: 0.9481638669967651\n",
            "Epoch: 34, Batch: 400, D Loss: 1.2568960189819336, G Loss: 0.8286648392677307\n",
            "Epoch: 34, Batch: 500, D Loss: 1.3358933925628662, G Loss: 0.7569338083267212\n",
            "Epoch: 34, Batch: 600, D Loss: 1.3497506380081177, G Loss: 0.8699196577072144\n",
            "Epoch: 34, Batch: 700, D Loss: 1.3643285036087036, G Loss: 0.9257446527481079\n",
            "Epoch: 34, Batch: 800, D Loss: 1.1964571475982666, G Loss: 0.9096280336380005\n",
            "Epoch: 34, Batch: 900, D Loss: 1.3060451745986938, G Loss: 1.032044768333435\n",
            "Epoch: 35, Batch: 0, D Loss: 1.3553321361541748, G Loss: 0.8620936870574951\n",
            "Epoch: 35, Batch: 100, D Loss: 1.2646565437316895, G Loss: 0.7967659831047058\n",
            "Epoch: 35, Batch: 200, D Loss: 1.321372389793396, G Loss: 0.838721513748169\n",
            "Epoch: 35, Batch: 300, D Loss: 1.2449166774749756, G Loss: 0.9067013263702393\n",
            "Epoch: 35, Batch: 400, D Loss: 1.1762707233428955, G Loss: 0.8307688236236572\n",
            "Epoch: 35, Batch: 500, D Loss: 1.3024797439575195, G Loss: 0.8139947652816772\n",
            "Epoch: 35, Batch: 600, D Loss: 1.3851101398468018, G Loss: 0.9448191523551941\n",
            "Epoch: 35, Batch: 700, D Loss: 1.2489628791809082, G Loss: 0.9083513021469116\n",
            "Epoch: 35, Batch: 800, D Loss: 1.3175441026687622, G Loss: 0.766099214553833\n",
            "Epoch: 35, Batch: 900, D Loss: 1.379249095916748, G Loss: 0.9408457279205322\n",
            "Epoch: 36, Batch: 0, D Loss: 1.3605471849441528, G Loss: 0.8408149480819702\n",
            "Epoch: 36, Batch: 100, D Loss: 1.2137478590011597, G Loss: 0.7389443516731262\n",
            "Epoch: 36, Batch: 200, D Loss: 1.2790567874908447, G Loss: 0.8261135816574097\n",
            "Epoch: 36, Batch: 300, D Loss: 1.2779390811920166, G Loss: 0.8634617924690247\n",
            "Epoch: 36, Batch: 400, D Loss: 1.305356740951538, G Loss: 0.7854130268096924\n",
            "Epoch: 36, Batch: 500, D Loss: 1.2581448554992676, G Loss: 0.8713940978050232\n",
            "Epoch: 36, Batch: 600, D Loss: 1.2295622825622559, G Loss: 0.83797687292099\n",
            "Epoch: 36, Batch: 700, D Loss: 1.2870588302612305, G Loss: 0.8150913119316101\n",
            "Epoch: 36, Batch: 800, D Loss: 1.3119442462921143, G Loss: 0.9000550508499146\n",
            "Epoch: 36, Batch: 900, D Loss: 1.326242208480835, G Loss: 0.9100759029388428\n",
            "Epoch: 37, Batch: 0, D Loss: 1.1707327365875244, G Loss: 0.9309253692626953\n",
            "Epoch: 37, Batch: 100, D Loss: 1.2226855754852295, G Loss: 0.8556606769561768\n",
            "Epoch: 37, Batch: 200, D Loss: 1.398172378540039, G Loss: 0.8952077031135559\n",
            "Epoch: 37, Batch: 300, D Loss: 1.2561931610107422, G Loss: 0.8115904927253723\n",
            "Epoch: 37, Batch: 400, D Loss: 1.2289097309112549, G Loss: 0.8332412242889404\n",
            "Epoch: 37, Batch: 500, D Loss: 1.245530366897583, G Loss: 0.9137609004974365\n",
            "Epoch: 37, Batch: 600, D Loss: 1.3276610374450684, G Loss: 0.8693322539329529\n",
            "Epoch: 37, Batch: 700, D Loss: 1.2780457735061646, G Loss: 0.8009629845619202\n",
            "Epoch: 37, Batch: 800, D Loss: 1.4028077125549316, G Loss: 0.8935649394989014\n",
            "Epoch: 37, Batch: 900, D Loss: 1.2270638942718506, G Loss: 0.8371244668960571\n",
            "Epoch: 38, Batch: 0, D Loss: 1.1937038898468018, G Loss: 0.8964143395423889\n",
            "Epoch: 38, Batch: 100, D Loss: 1.2425285577774048, G Loss: 0.8273760676383972\n",
            "Epoch: 38, Batch: 200, D Loss: 1.1082780361175537, G Loss: 0.8880379796028137\n",
            "Epoch: 38, Batch: 300, D Loss: 1.2777512073516846, G Loss: 0.8963296413421631\n",
            "Epoch: 38, Batch: 400, D Loss: 1.0950512886047363, G Loss: 1.0194666385650635\n",
            "Epoch: 38, Batch: 500, D Loss: 1.3616704940795898, G Loss: 0.8316746950149536\n",
            "Epoch: 38, Batch: 600, D Loss: 1.269026517868042, G Loss: 0.8711058497428894\n",
            "Epoch: 38, Batch: 700, D Loss: 1.204344630241394, G Loss: 0.9235480427742004\n",
            "Epoch: 38, Batch: 800, D Loss: 1.2557029724121094, G Loss: 0.8581389784812927\n",
            "Epoch: 38, Batch: 900, D Loss: 1.169785499572754, G Loss: 1.0402121543884277\n",
            "Epoch: 39, Batch: 0, D Loss: 1.214263916015625, G Loss: 0.8604550957679749\n",
            "Epoch: 39, Batch: 100, D Loss: 1.2354984283447266, G Loss: 0.877760648727417\n",
            "Epoch: 39, Batch: 200, D Loss: 1.3152070045471191, G Loss: 0.8497074246406555\n",
            "Epoch: 39, Batch: 300, D Loss: 1.3809529542922974, G Loss: 0.9386661648750305\n",
            "Epoch: 39, Batch: 400, D Loss: 1.243535041809082, G Loss: 0.9399665594100952\n",
            "Epoch: 39, Batch: 500, D Loss: 1.237391471862793, G Loss: 0.8394979238510132\n",
            "Epoch: 39, Batch: 600, D Loss: 1.3567147254943848, G Loss: 0.8284547328948975\n",
            "Epoch: 39, Batch: 700, D Loss: 1.2559260129928589, G Loss: 0.885684609413147\n",
            "Epoch: 39, Batch: 800, D Loss: 1.3001718521118164, G Loss: 0.8371813297271729\n",
            "Epoch: 39, Batch: 900, D Loss: 1.3067420721054077, G Loss: 0.9132066369056702\n",
            "Epoch: 40, Batch: 0, D Loss: 1.2786719799041748, G Loss: 0.9442415237426758\n",
            "Epoch: 40, Batch: 100, D Loss: 1.2812206745147705, G Loss: 0.9185973405838013\n",
            "Epoch: 40, Batch: 200, D Loss: 1.1943633556365967, G Loss: 0.9777427911758423\n",
            "Epoch: 40, Batch: 300, D Loss: 1.2816414833068848, G Loss: 0.9398441314697266\n",
            "Epoch: 40, Batch: 400, D Loss: 1.2126944065093994, G Loss: 0.8523203730583191\n",
            "Epoch: 40, Batch: 500, D Loss: 1.2037643194198608, G Loss: 1.0083891153335571\n",
            "Epoch: 40, Batch: 600, D Loss: 1.3450419902801514, G Loss: 0.9256958365440369\n",
            "Epoch: 40, Batch: 700, D Loss: 1.2612216472625732, G Loss: 0.9679707288742065\n",
            "Epoch: 40, Batch: 800, D Loss: 1.3241664171218872, G Loss: 0.7792202234268188\n",
            "Epoch: 40, Batch: 900, D Loss: 1.254258394241333, G Loss: 0.8354804515838623\n",
            "Epoch: 41, Batch: 0, D Loss: 1.324197769165039, G Loss: 0.8825225234031677\n",
            "Epoch: 41, Batch: 100, D Loss: 1.2558503150939941, G Loss: 0.8751910924911499\n",
            "Epoch: 41, Batch: 200, D Loss: 1.2546486854553223, G Loss: 0.9003717303276062\n",
            "Epoch: 41, Batch: 300, D Loss: 1.2929999828338623, G Loss: 0.9475241303443909\n",
            "Epoch: 41, Batch: 400, D Loss: 1.166435718536377, G Loss: 0.825150191783905\n",
            "Epoch: 41, Batch: 500, D Loss: 1.2901313304901123, G Loss: 0.938387930393219\n",
            "Epoch: 41, Batch: 600, D Loss: 1.2916241884231567, G Loss: 0.8537483215332031\n",
            "Epoch: 41, Batch: 700, D Loss: 1.216817855834961, G Loss: 0.8447248935699463\n",
            "Epoch: 41, Batch: 800, D Loss: 1.3229684829711914, G Loss: 0.9958050847053528\n",
            "Epoch: 41, Batch: 900, D Loss: 1.3704817295074463, G Loss: 1.0117714405059814\n",
            "Epoch: 42, Batch: 0, D Loss: 1.2428312301635742, G Loss: 0.9304646253585815\n",
            "Epoch: 42, Batch: 100, D Loss: 1.2443327903747559, G Loss: 0.8260098695755005\n",
            "Epoch: 42, Batch: 200, D Loss: 1.2042070627212524, G Loss: 0.9111534953117371\n",
            "Epoch: 42, Batch: 300, D Loss: 1.2459568977355957, G Loss: 0.9836640954017639\n",
            "Epoch: 42, Batch: 400, D Loss: 1.3010988235473633, G Loss: 0.9548247456550598\n",
            "Epoch: 42, Batch: 500, D Loss: 1.297168493270874, G Loss: 0.9052665829658508\n",
            "Epoch: 42, Batch: 600, D Loss: 1.217529058456421, G Loss: 0.8169555068016052\n",
            "Epoch: 42, Batch: 700, D Loss: 1.2305998802185059, G Loss: 0.8670009970664978\n",
            "Epoch: 42, Batch: 800, D Loss: 1.2270216941833496, G Loss: 0.8718605041503906\n",
            "Epoch: 42, Batch: 900, D Loss: 1.3417589664459229, G Loss: 0.9200326800346375\n",
            "Epoch: 43, Batch: 0, D Loss: 1.3864665031433105, G Loss: 0.8405182361602783\n",
            "Epoch: 43, Batch: 100, D Loss: 1.2938332557678223, G Loss: 0.8737236261367798\n",
            "Epoch: 43, Batch: 200, D Loss: 1.2168002128601074, G Loss: 0.9058075547218323\n",
            "Epoch: 43, Batch: 300, D Loss: 1.2334637641906738, G Loss: 0.7650811672210693\n",
            "Epoch: 43, Batch: 400, D Loss: 1.1484804153442383, G Loss: 0.8907750844955444\n",
            "Epoch: 43, Batch: 500, D Loss: 1.281821608543396, G Loss: 1.0625536441802979\n",
            "Epoch: 43, Batch: 600, D Loss: 1.2675788402557373, G Loss: 0.9476953148841858\n",
            "Epoch: 43, Batch: 700, D Loss: 1.2591629028320312, G Loss: 0.9271532893180847\n",
            "Epoch: 43, Batch: 800, D Loss: 1.328977346420288, G Loss: 0.7746813893318176\n",
            "Epoch: 43, Batch: 900, D Loss: 1.3115766048431396, G Loss: 0.8853991031646729\n",
            "Epoch: 44, Batch: 0, D Loss: 1.331093192100525, G Loss: 0.8532024621963501\n",
            "Epoch: 44, Batch: 100, D Loss: 1.3660681247711182, G Loss: 0.8110382556915283\n",
            "Epoch: 44, Batch: 200, D Loss: 1.3588765859603882, G Loss: 1.0641467571258545\n",
            "Epoch: 44, Batch: 300, D Loss: 1.2196141481399536, G Loss: 0.7981575131416321\n",
            "Epoch: 44, Batch: 400, D Loss: 1.336510181427002, G Loss: 0.8909146785736084\n",
            "Epoch: 44, Batch: 500, D Loss: 1.2795392274856567, G Loss: 0.861697793006897\n",
            "Epoch: 44, Batch: 600, D Loss: 1.2253198623657227, G Loss: 0.816856324672699\n",
            "Epoch: 44, Batch: 700, D Loss: 1.3070852756500244, G Loss: 0.9482152462005615\n",
            "Epoch: 44, Batch: 800, D Loss: 1.3115389347076416, G Loss: 0.7121474146842957\n",
            "Epoch: 44, Batch: 900, D Loss: 1.2350406646728516, G Loss: 0.9543460607528687\n",
            "Epoch: 45, Batch: 0, D Loss: 1.3491030931472778, G Loss: 0.8899682760238647\n",
            "Epoch: 45, Batch: 100, D Loss: 1.257370948791504, G Loss: 0.8506895899772644\n",
            "Epoch: 45, Batch: 200, D Loss: 1.2543871402740479, G Loss: 0.9670610427856445\n",
            "Epoch: 45, Batch: 300, D Loss: 1.2571736574172974, G Loss: 0.913672149181366\n",
            "Epoch: 45, Batch: 400, D Loss: 1.1410713195800781, G Loss: 0.8486388921737671\n",
            "Epoch: 45, Batch: 500, D Loss: 1.2674708366394043, G Loss: 0.8254714608192444\n",
            "Epoch: 45, Batch: 600, D Loss: 1.241337537765503, G Loss: 0.9098265767097473\n",
            "Epoch: 45, Batch: 700, D Loss: 1.2937684059143066, G Loss: 0.9674989581108093\n",
            "Epoch: 45, Batch: 800, D Loss: 1.27399742603302, G Loss: 0.8962725400924683\n",
            "Epoch: 45, Batch: 900, D Loss: 1.1386137008666992, G Loss: 0.8992622494697571\n",
            "Epoch: 46, Batch: 0, D Loss: 1.2881817817687988, G Loss: 0.8925921320915222\n",
            "Epoch: 46, Batch: 100, D Loss: 1.3790748119354248, G Loss: 0.8913490772247314\n",
            "Epoch: 46, Batch: 200, D Loss: 1.2420765161514282, G Loss: 0.8460184931755066\n",
            "Epoch: 46, Batch: 300, D Loss: 1.3675950765609741, G Loss: 0.8593158721923828\n",
            "Epoch: 46, Batch: 400, D Loss: 1.2671210765838623, G Loss: 0.8804375529289246\n",
            "Epoch: 46, Batch: 500, D Loss: 1.2969746589660645, G Loss: 0.8163254261016846\n",
            "Epoch: 46, Batch: 600, D Loss: 1.157930850982666, G Loss: 0.8563642501831055\n",
            "Epoch: 46, Batch: 700, D Loss: 1.328972578048706, G Loss: 0.9201644659042358\n",
            "Epoch: 46, Batch: 800, D Loss: 1.2242786884307861, G Loss: 0.8665598034858704\n",
            "Epoch: 46, Batch: 900, D Loss: 1.258819818496704, G Loss: 0.7653536200523376\n",
            "Epoch: 47, Batch: 0, D Loss: 1.342602014541626, G Loss: 0.9436894655227661\n",
            "Epoch: 47, Batch: 100, D Loss: 1.2151217460632324, G Loss: 0.9362362623214722\n",
            "Epoch: 47, Batch: 200, D Loss: 1.1892194747924805, G Loss: 0.8839303255081177\n",
            "Epoch: 47, Batch: 300, D Loss: 1.328991174697876, G Loss: 0.9653996825218201\n",
            "Epoch: 47, Batch: 400, D Loss: 1.3167874813079834, G Loss: 1.0148124694824219\n",
            "Epoch: 47, Batch: 500, D Loss: 1.2766581773757935, G Loss: 0.8058977723121643\n",
            "Epoch: 47, Batch: 600, D Loss: 1.3575859069824219, G Loss: 0.9779289960861206\n",
            "Epoch: 47, Batch: 700, D Loss: 1.2277288436889648, G Loss: 0.8275929093360901\n",
            "Epoch: 47, Batch: 800, D Loss: 1.210371494293213, G Loss: 0.8746966123580933\n",
            "Epoch: 47, Batch: 900, D Loss: 1.2577917575836182, G Loss: 0.7807375192642212\n",
            "Epoch: 48, Batch: 0, D Loss: 1.2562668323516846, G Loss: 0.8462608456611633\n",
            "Epoch: 48, Batch: 100, D Loss: 1.2820768356323242, G Loss: 0.8468160033226013\n",
            "Epoch: 48, Batch: 200, D Loss: 1.2472007274627686, G Loss: 0.7804065942764282\n",
            "Epoch: 48, Batch: 300, D Loss: 1.403596043586731, G Loss: 0.7861385941505432\n",
            "Epoch: 48, Batch: 400, D Loss: 1.2382822036743164, G Loss: 0.8418341875076294\n",
            "Epoch: 48, Batch: 500, D Loss: 1.3180961608886719, G Loss: 0.9000940322875977\n",
            "Epoch: 48, Batch: 600, D Loss: 1.265376091003418, G Loss: 0.8692765831947327\n",
            "Epoch: 48, Batch: 700, D Loss: 1.2851674556732178, G Loss: 0.8670398592948914\n",
            "Epoch: 48, Batch: 800, D Loss: 1.2208123207092285, G Loss: 0.9641422033309937\n",
            "Epoch: 48, Batch: 900, D Loss: 1.1709957122802734, G Loss: 0.796441912651062\n",
            "Epoch: 49, Batch: 0, D Loss: 1.2258087396621704, G Loss: 0.7334994077682495\n",
            "Epoch: 49, Batch: 100, D Loss: 1.2606735229492188, G Loss: 0.891412615776062\n",
            "Epoch: 49, Batch: 200, D Loss: 1.233367681503296, G Loss: 0.9240753054618835\n",
            "Epoch: 49, Batch: 300, D Loss: 1.273423433303833, G Loss: 0.7824373841285706\n",
            "Epoch: 49, Batch: 400, D Loss: 1.2366814613342285, G Loss: 0.7879213094711304\n",
            "Epoch: 49, Batch: 500, D Loss: 1.2796177864074707, G Loss: 0.8668152689933777\n",
            "Epoch: 49, Batch: 600, D Loss: 1.168156623840332, G Loss: 0.9183136224746704\n",
            "Epoch: 49, Batch: 700, D Loss: 1.268883228302002, G Loss: 0.9134008288383484\n",
            "Epoch: 49, Batch: 800, D Loss: 1.2444045543670654, G Loss: 0.8271310329437256\n",
            "Epoch: 49, Batch: 900, D Loss: 1.1946889162063599, G Loss: 0.8005779981613159\n",
            "Epoch: 50, Batch: 0, D Loss: 1.3372297286987305, G Loss: 0.9279870986938477\n",
            "Epoch: 50, Batch: 100, D Loss: 1.3525440692901611, G Loss: 1.0412518978118896\n",
            "Epoch: 50, Batch: 200, D Loss: 1.2693853378295898, G Loss: 0.8114464282989502\n",
            "Epoch: 50, Batch: 300, D Loss: 1.2331726551055908, G Loss: 0.868027925491333\n",
            "Epoch: 50, Batch: 400, D Loss: 1.2500180006027222, G Loss: 0.877784788608551\n",
            "Epoch: 50, Batch: 500, D Loss: 1.310140609741211, G Loss: 0.8285278081893921\n",
            "Epoch: 50, Batch: 600, D Loss: 1.2673262357711792, G Loss: 0.8438465595245361\n",
            "Epoch: 50, Batch: 700, D Loss: 1.421025276184082, G Loss: 0.9717885851860046\n",
            "Epoch: 50, Batch: 800, D Loss: 1.2613965272903442, G Loss: 0.7862001061439514\n",
            "Epoch: 50, Batch: 900, D Loss: 1.2280882596969604, G Loss: 0.8345898389816284\n",
            "Epoch: 51, Batch: 0, D Loss: 1.1950119733810425, G Loss: 0.8842834234237671\n",
            "Epoch: 51, Batch: 100, D Loss: 1.196938395500183, G Loss: 1.0352543592453003\n",
            "Epoch: 51, Batch: 200, D Loss: 1.3728526830673218, G Loss: 0.9234206676483154\n",
            "Epoch: 51, Batch: 300, D Loss: 1.320244550704956, G Loss: 0.8856872916221619\n",
            "Epoch: 51, Batch: 400, D Loss: 1.3532406091690063, G Loss: 0.8135420680046082\n",
            "Epoch: 51, Batch: 500, D Loss: 1.263458013534546, G Loss: 0.8711034059524536\n",
            "Epoch: 51, Batch: 600, D Loss: 1.2551946640014648, G Loss: 1.0272191762924194\n",
            "Epoch: 51, Batch: 700, D Loss: 1.4628454446792603, G Loss: 0.7989904284477234\n",
            "Epoch: 51, Batch: 800, D Loss: 1.3135919570922852, G Loss: 0.9523801207542419\n",
            "Epoch: 51, Batch: 900, D Loss: 1.2036107778549194, G Loss: 0.9103108644485474\n",
            "Epoch: 52, Batch: 0, D Loss: 1.1741617918014526, G Loss: 0.8127478957176208\n",
            "Epoch: 52, Batch: 100, D Loss: 1.2851835489273071, G Loss: 0.8203710913658142\n",
            "Epoch: 52, Batch: 200, D Loss: 1.241416096687317, G Loss: 0.8469066023826599\n",
            "Epoch: 52, Batch: 300, D Loss: 1.3043031692504883, G Loss: 0.9510898590087891\n",
            "Epoch: 52, Batch: 400, D Loss: 1.265564203262329, G Loss: 0.8213518857955933\n",
            "Epoch: 52, Batch: 500, D Loss: 1.2878892421722412, G Loss: 0.8589484095573425\n",
            "Epoch: 52, Batch: 600, D Loss: 1.25260329246521, G Loss: 0.8752697110176086\n",
            "Epoch: 52, Batch: 700, D Loss: 1.2850642204284668, G Loss: 0.8954283595085144\n",
            "Epoch: 52, Batch: 800, D Loss: 1.3038201332092285, G Loss: 0.8735847473144531\n",
            "Epoch: 52, Batch: 900, D Loss: 1.2937240600585938, G Loss: 0.8612002730369568\n",
            "Epoch: 53, Batch: 0, D Loss: 1.2118375301361084, G Loss: 0.8942453265190125\n",
            "Epoch: 53, Batch: 100, D Loss: 1.3027303218841553, G Loss: 0.9618688225746155\n",
            "Epoch: 53, Batch: 200, D Loss: 1.2801095247268677, G Loss: 0.8938729166984558\n",
            "Epoch: 53, Batch: 300, D Loss: 1.2048325538635254, G Loss: 1.0445915460586548\n",
            "Epoch: 53, Batch: 400, D Loss: 1.2949926853179932, G Loss: 0.8486204743385315\n",
            "Epoch: 53, Batch: 500, D Loss: 1.3075001239776611, G Loss: 1.010076880455017\n",
            "Epoch: 53, Batch: 600, D Loss: 1.2717171907424927, G Loss: 0.8043894171714783\n",
            "Epoch: 53, Batch: 700, D Loss: 1.3054345846176147, G Loss: 0.9712165594100952\n",
            "Epoch: 53, Batch: 800, D Loss: 1.2533771991729736, G Loss: 0.8927455544471741\n",
            "Epoch: 53, Batch: 900, D Loss: 1.425281047821045, G Loss: 0.9355993270874023\n",
            "Epoch: 54, Batch: 0, D Loss: 1.2034927606582642, G Loss: 0.8295684456825256\n",
            "Epoch: 54, Batch: 100, D Loss: 1.25880765914917, G Loss: 0.8165656924247742\n",
            "Epoch: 54, Batch: 200, D Loss: 1.1992154121398926, G Loss: 0.9518524408340454\n",
            "Epoch: 54, Batch: 300, D Loss: 1.3374953269958496, G Loss: 0.8884696364402771\n",
            "Epoch: 54, Batch: 400, D Loss: 1.2184520959854126, G Loss: 0.8655994534492493\n",
            "Epoch: 54, Batch: 500, D Loss: 1.2605838775634766, G Loss: 0.9582401514053345\n",
            "Epoch: 54, Batch: 600, D Loss: 1.3882585763931274, G Loss: 0.8014437556266785\n",
            "Epoch: 54, Batch: 700, D Loss: 1.3407764434814453, G Loss: 0.8786910772323608\n",
            "Epoch: 54, Batch: 800, D Loss: 1.3166906833648682, G Loss: 0.689878523349762\n",
            "Epoch: 54, Batch: 900, D Loss: 1.2337806224822998, G Loss: 0.8132635951042175\n",
            "Epoch: 55, Batch: 0, D Loss: 1.184692621231079, G Loss: 0.9484094977378845\n",
            "Epoch: 55, Batch: 100, D Loss: 1.1676199436187744, G Loss: 0.7725992202758789\n",
            "Epoch: 55, Batch: 200, D Loss: 1.3384732007980347, G Loss: 0.8657694458961487\n",
            "Epoch: 55, Batch: 300, D Loss: 1.3324192762374878, G Loss: 0.8323631882667542\n",
            "Epoch: 55, Batch: 400, D Loss: 1.2445693016052246, G Loss: 0.9742815494537354\n",
            "Epoch: 55, Batch: 500, D Loss: 1.1455785036087036, G Loss: 0.9510878324508667\n",
            "Epoch: 55, Batch: 600, D Loss: 1.293968915939331, G Loss: 0.9467368721961975\n",
            "Epoch: 55, Batch: 700, D Loss: 1.1838585138320923, G Loss: 0.8543005585670471\n",
            "Epoch: 55, Batch: 800, D Loss: 1.2359287738800049, G Loss: 0.7115001678466797\n",
            "Epoch: 55, Batch: 900, D Loss: 1.260575532913208, G Loss: 0.7276230454444885\n",
            "Epoch: 56, Batch: 0, D Loss: 1.2993075847625732, G Loss: 0.9514188170433044\n",
            "Epoch: 56, Batch: 100, D Loss: 1.206921100616455, G Loss: 0.7794626355171204\n",
            "Epoch: 56, Batch: 200, D Loss: 1.4065587520599365, G Loss: 0.8820037841796875\n",
            "Epoch: 56, Batch: 300, D Loss: 1.3025838136672974, G Loss: 0.9069817066192627\n",
            "Epoch: 56, Batch: 400, D Loss: 1.2026431560516357, G Loss: 0.926903486251831\n",
            "Epoch: 56, Batch: 500, D Loss: 1.2824312448501587, G Loss: 0.7881506085395813\n",
            "Epoch: 56, Batch: 600, D Loss: 1.3213070631027222, G Loss: 0.8635291457176208\n",
            "Epoch: 56, Batch: 700, D Loss: 1.1950678825378418, G Loss: 0.8800720572471619\n",
            "Epoch: 56, Batch: 800, D Loss: 1.3421623706817627, G Loss: 0.7895933985710144\n",
            "Epoch: 56, Batch: 900, D Loss: 1.3129647970199585, G Loss: 0.7973519563674927\n",
            "Epoch: 57, Batch: 0, D Loss: 1.2538046836853027, G Loss: 0.9013935923576355\n",
            "Epoch: 57, Batch: 100, D Loss: 1.257420301437378, G Loss: 0.8732258677482605\n",
            "Epoch: 57, Batch: 200, D Loss: 1.3312082290649414, G Loss: 0.9209288358688354\n",
            "Epoch: 57, Batch: 300, D Loss: 1.2743459939956665, G Loss: 1.0321568250656128\n",
            "Epoch: 57, Batch: 400, D Loss: 1.2980010509490967, G Loss: 0.9676101207733154\n",
            "Epoch: 57, Batch: 500, D Loss: 1.2009027004241943, G Loss: 0.8133700489997864\n",
            "Epoch: 57, Batch: 600, D Loss: 1.344984531402588, G Loss: 0.8731719851493835\n",
            "Epoch: 57, Batch: 700, D Loss: 1.3347923755645752, G Loss: 0.9358921051025391\n",
            "Epoch: 57, Batch: 800, D Loss: 1.3731751441955566, G Loss: 0.8762709498405457\n",
            "Epoch: 57, Batch: 900, D Loss: 1.2908756732940674, G Loss: 0.9494490027427673\n",
            "Epoch: 58, Batch: 0, D Loss: 1.2314932346343994, G Loss: 0.8530405759811401\n",
            "Epoch: 58, Batch: 100, D Loss: 1.225285530090332, G Loss: 0.8352406620979309\n",
            "Epoch: 58, Batch: 200, D Loss: 1.2920188903808594, G Loss: 0.837325394153595\n",
            "Epoch: 58, Batch: 300, D Loss: 1.2420847415924072, G Loss: 0.8891037702560425\n",
            "Epoch: 58, Batch: 400, D Loss: 1.2681859731674194, G Loss: 0.8299259543418884\n",
            "Epoch: 58, Batch: 500, D Loss: 1.3237558603286743, G Loss: 0.886806845664978\n",
            "Epoch: 58, Batch: 600, D Loss: 1.2373913526535034, G Loss: 0.9368077516555786\n",
            "Epoch: 58, Batch: 700, D Loss: 1.2307286262512207, G Loss: 0.8573797345161438\n",
            "Epoch: 58, Batch: 800, D Loss: 1.278253436088562, G Loss: 0.8607604503631592\n",
            "Epoch: 58, Batch: 900, D Loss: 1.3440253734588623, G Loss: 0.8812716007232666\n",
            "Epoch: 59, Batch: 0, D Loss: 1.309150218963623, G Loss: 0.8586922883987427\n",
            "Epoch: 59, Batch: 100, D Loss: 1.2267212867736816, G Loss: 0.8508586883544922\n",
            "Epoch: 59, Batch: 200, D Loss: 1.299773097038269, G Loss: 0.8894960284233093\n",
            "Epoch: 59, Batch: 300, D Loss: 1.256788969039917, G Loss: 0.8739656805992126\n",
            "Epoch: 59, Batch: 400, D Loss: 1.266862154006958, G Loss: 0.9434893131256104\n",
            "Epoch: 59, Batch: 500, D Loss: 1.2587692737579346, G Loss: 0.942616879940033\n",
            "Epoch: 59, Batch: 600, D Loss: 1.4004251956939697, G Loss: 0.8941619992256165\n",
            "Epoch: 59, Batch: 700, D Loss: 1.3572793006896973, G Loss: 0.8582550287246704\n",
            "Epoch: 59, Batch: 800, D Loss: 1.3056678771972656, G Loss: 0.8047996759414673\n",
            "Epoch: 59, Batch: 900, D Loss: 1.2626218795776367, G Loss: 0.9668149352073669\n",
            "Epoch: 60, Batch: 0, D Loss: 1.2548770904541016, G Loss: 0.8627740144729614\n",
            "Epoch: 60, Batch: 100, D Loss: 1.2677924633026123, G Loss: 1.0356847047805786\n",
            "Epoch: 60, Batch: 200, D Loss: 1.3093478679656982, G Loss: 0.8527056574821472\n",
            "Epoch: 60, Batch: 300, D Loss: 1.2278939485549927, G Loss: 0.824388861656189\n",
            "Epoch: 60, Batch: 400, D Loss: 1.2942874431610107, G Loss: 0.9517262578010559\n",
            "Epoch: 60, Batch: 500, D Loss: 1.2072384357452393, G Loss: 0.8723329305648804\n",
            "Epoch: 60, Batch: 600, D Loss: 1.3008191585540771, G Loss: 0.9586218595504761\n",
            "Epoch: 60, Batch: 700, D Loss: 1.2237086296081543, G Loss: 0.936837375164032\n",
            "Epoch: 60, Batch: 800, D Loss: 1.2609732151031494, G Loss: 0.8004430532455444\n",
            "Epoch: 60, Batch: 900, D Loss: 1.243648648262024, G Loss: 0.9342288970947266\n",
            "Epoch: 61, Batch: 0, D Loss: 1.3026808500289917, G Loss: 0.8056899905204773\n",
            "Epoch: 61, Batch: 100, D Loss: 1.3321504592895508, G Loss: 0.8608525395393372\n",
            "Epoch: 61, Batch: 200, D Loss: 1.2239210605621338, G Loss: 0.8212834596633911\n",
            "Epoch: 61, Batch: 300, D Loss: 1.257807731628418, G Loss: 0.80223548412323\n",
            "Epoch: 61, Batch: 400, D Loss: 1.230592131614685, G Loss: 0.8488917946815491\n",
            "Epoch: 61, Batch: 500, D Loss: 1.2388814687728882, G Loss: 0.7778604030609131\n",
            "Epoch: 61, Batch: 600, D Loss: 1.2007033824920654, G Loss: 0.968443751335144\n",
            "Epoch: 61, Batch: 700, D Loss: 1.3241486549377441, G Loss: 0.8893991708755493\n",
            "Epoch: 61, Batch: 800, D Loss: 1.260871410369873, G Loss: 0.889100968837738\n",
            "Epoch: 61, Batch: 900, D Loss: 1.1424179077148438, G Loss: 0.7669054865837097\n",
            "Epoch: 62, Batch: 0, D Loss: 1.3390851020812988, G Loss: 0.9983834624290466\n",
            "Epoch: 62, Batch: 100, D Loss: 1.3249459266662598, G Loss: 0.7289058566093445\n",
            "Epoch: 62, Batch: 200, D Loss: 1.2135989665985107, G Loss: 0.9329636693000793\n",
            "Epoch: 62, Batch: 300, D Loss: 1.3478503227233887, G Loss: 0.9614898562431335\n",
            "Epoch: 62, Batch: 400, D Loss: 1.2675549983978271, G Loss: 0.9888949990272522\n",
            "Epoch: 62, Batch: 500, D Loss: 1.2761235237121582, G Loss: 0.8700050115585327\n",
            "Epoch: 62, Batch: 600, D Loss: 1.3448410034179688, G Loss: 0.9101907014846802\n",
            "Epoch: 62, Batch: 700, D Loss: 1.4808130264282227, G Loss: 0.8393846154212952\n",
            "Epoch: 62, Batch: 800, D Loss: 1.262144684791565, G Loss: 0.7952529191970825\n",
            "Epoch: 62, Batch: 900, D Loss: 1.244231939315796, G Loss: 0.8774033784866333\n",
            "Epoch: 63, Batch: 0, D Loss: 1.3477798700332642, G Loss: 0.9900976419448853\n",
            "Epoch: 63, Batch: 100, D Loss: 1.370631456375122, G Loss: 0.8766705989837646\n",
            "Epoch: 63, Batch: 200, D Loss: 1.208327054977417, G Loss: 0.8223822116851807\n",
            "Epoch: 63, Batch: 300, D Loss: 1.1671380996704102, G Loss: 0.7679425477981567\n",
            "Epoch: 63, Batch: 400, D Loss: 1.4045112133026123, G Loss: 0.8139536380767822\n",
            "Epoch: 63, Batch: 500, D Loss: 1.2774759531021118, G Loss: 0.9018462896347046\n",
            "Epoch: 63, Batch: 600, D Loss: 1.2200651168823242, G Loss: 0.8303021192550659\n",
            "Epoch: 63, Batch: 700, D Loss: 1.2577567100524902, G Loss: 0.8319387435913086\n",
            "Epoch: 63, Batch: 800, D Loss: 1.2444276809692383, G Loss: 0.883613646030426\n",
            "Epoch: 63, Batch: 900, D Loss: 1.2157516479492188, G Loss: 1.004284381866455\n",
            "Epoch: 64, Batch: 0, D Loss: 1.2941114902496338, G Loss: 0.9301234483718872\n",
            "Epoch: 64, Batch: 100, D Loss: 1.2638778686523438, G Loss: 0.833727240562439\n",
            "Epoch: 64, Batch: 200, D Loss: 1.2068901062011719, G Loss: 0.834581732749939\n",
            "Epoch: 64, Batch: 300, D Loss: 1.2620929479599, G Loss: 0.9282751083374023\n",
            "Epoch: 64, Batch: 400, D Loss: 1.200510025024414, G Loss: 0.9635252356529236\n",
            "Epoch: 64, Batch: 500, D Loss: 1.307499885559082, G Loss: 0.8604233860969543\n",
            "Epoch: 64, Batch: 600, D Loss: 1.4078807830810547, G Loss: 0.8408419489860535\n",
            "Epoch: 64, Batch: 700, D Loss: 1.2076053619384766, G Loss: 0.8128222227096558\n",
            "Epoch: 64, Batch: 800, D Loss: 1.2935822010040283, G Loss: 0.8862110376358032\n",
            "Epoch: 64, Batch: 900, D Loss: 1.4493944644927979, G Loss: 0.9080079197883606\n",
            "Epoch: 65, Batch: 0, D Loss: 1.236271619796753, G Loss: 0.8457301259040833\n",
            "Epoch: 65, Batch: 100, D Loss: 1.2940629720687866, G Loss: 0.7969629168510437\n",
            "Epoch: 65, Batch: 200, D Loss: 1.313887596130371, G Loss: 0.831981897354126\n",
            "Epoch: 65, Batch: 300, D Loss: 1.2137680053710938, G Loss: 0.8188360333442688\n",
            "Epoch: 65, Batch: 400, D Loss: 1.3125739097595215, G Loss: 0.9044917821884155\n",
            "Epoch: 65, Batch: 500, D Loss: 1.2534605264663696, G Loss: 0.8324660062789917\n",
            "Epoch: 65, Batch: 600, D Loss: 1.2958364486694336, G Loss: 0.7920545339584351\n",
            "Epoch: 65, Batch: 700, D Loss: 1.3323214054107666, G Loss: 0.9335687756538391\n",
            "Epoch: 65, Batch: 800, D Loss: 1.2778606414794922, G Loss: 0.9626100063323975\n",
            "Epoch: 65, Batch: 900, D Loss: 1.1300575733184814, G Loss: 0.8980904817581177\n",
            "Epoch: 66, Batch: 0, D Loss: 1.354332447052002, G Loss: 0.8439894914627075\n",
            "Epoch: 66, Batch: 100, D Loss: 1.3136301040649414, G Loss: 0.8341595530509949\n",
            "Epoch: 66, Batch: 200, D Loss: 1.2008275985717773, G Loss: 0.8669629693031311\n",
            "Epoch: 66, Batch: 300, D Loss: 1.3345425128936768, G Loss: 1.0552666187286377\n",
            "Epoch: 66, Batch: 400, D Loss: 1.17264723777771, G Loss: 0.7972410321235657\n",
            "Epoch: 66, Batch: 500, D Loss: 1.2184135913848877, G Loss: 0.8515118956565857\n",
            "Epoch: 66, Batch: 600, D Loss: 1.2795519828796387, G Loss: 0.8561659455299377\n",
            "Epoch: 66, Batch: 700, D Loss: 1.14942467212677, G Loss: 0.9477998614311218\n",
            "Epoch: 66, Batch: 800, D Loss: 1.2805510759353638, G Loss: 0.8147766590118408\n",
            "Epoch: 66, Batch: 900, D Loss: 1.2979369163513184, G Loss: 0.773443877696991\n",
            "Epoch: 67, Batch: 0, D Loss: 1.1977250576019287, G Loss: 0.786001980304718\n",
            "Epoch: 67, Batch: 100, D Loss: 1.271388292312622, G Loss: 0.9430862665176392\n",
            "Epoch: 67, Batch: 200, D Loss: 1.2276804447174072, G Loss: 0.8130479454994202\n",
            "Epoch: 67, Batch: 300, D Loss: 1.232304573059082, G Loss: 0.7528879642486572\n",
            "Epoch: 67, Batch: 400, D Loss: 1.373107671737671, G Loss: 0.9969515204429626\n",
            "Epoch: 67, Batch: 500, D Loss: 1.349683165550232, G Loss: 0.9177181124687195\n",
            "Epoch: 67, Batch: 600, D Loss: 1.258232831954956, G Loss: 0.803108274936676\n",
            "Epoch: 67, Batch: 700, D Loss: 1.328047752380371, G Loss: 1.0749211311340332\n",
            "Epoch: 67, Batch: 800, D Loss: 1.2476208209991455, G Loss: 0.7999258637428284\n",
            "Epoch: 67, Batch: 900, D Loss: 1.297098159790039, G Loss: 0.8566367626190186\n",
            "Epoch: 68, Batch: 0, D Loss: 1.354997992515564, G Loss: 0.8666752576828003\n",
            "Epoch: 68, Batch: 100, D Loss: 1.2843103408813477, G Loss: 0.957760214805603\n",
            "Epoch: 68, Batch: 200, D Loss: 1.3174316883087158, G Loss: 0.9156845211982727\n",
            "Epoch: 68, Batch: 300, D Loss: 1.2491710186004639, G Loss: 0.8179986476898193\n",
            "Epoch: 68, Batch: 400, D Loss: 1.3630528450012207, G Loss: 0.757182240486145\n",
            "Epoch: 68, Batch: 500, D Loss: 1.280278205871582, G Loss: 0.8096330165863037\n",
            "Epoch: 68, Batch: 600, D Loss: 1.275701880455017, G Loss: 0.735883355140686\n",
            "Epoch: 68, Batch: 700, D Loss: 1.182847499847412, G Loss: 0.9072274565696716\n",
            "Epoch: 68, Batch: 800, D Loss: 1.2605066299438477, G Loss: 0.9720161557197571\n",
            "Epoch: 68, Batch: 900, D Loss: 1.2428239583969116, G Loss: 0.7816658616065979\n",
            "Epoch: 69, Batch: 0, D Loss: 1.3247445821762085, G Loss: 0.8942118883132935\n",
            "Epoch: 69, Batch: 100, D Loss: 1.3646538257598877, G Loss: 0.9513286352157593\n",
            "Epoch: 69, Batch: 200, D Loss: 1.2541182041168213, G Loss: 0.863493800163269\n",
            "Epoch: 69, Batch: 300, D Loss: 1.2091083526611328, G Loss: 0.8662994503974915\n",
            "Epoch: 69, Batch: 400, D Loss: 1.1766357421875, G Loss: 0.9214504957199097\n",
            "Epoch: 69, Batch: 500, D Loss: 1.294152021408081, G Loss: 0.9354925751686096\n",
            "Epoch: 69, Batch: 600, D Loss: 1.3887795209884644, G Loss: 0.7702768445014954\n",
            "Epoch: 69, Batch: 700, D Loss: 1.2367571592330933, G Loss: 0.8816940188407898\n",
            "Epoch: 69, Batch: 800, D Loss: 1.2783334255218506, G Loss: 0.9531992077827454\n",
            "Epoch: 69, Batch: 900, D Loss: 1.2430109977722168, G Loss: 0.9095245599746704\n",
            "Epoch: 70, Batch: 0, D Loss: 1.173190712928772, G Loss: 0.9266151785850525\n",
            "Epoch: 70, Batch: 100, D Loss: 1.3471633195877075, G Loss: 0.8720957040786743\n",
            "Epoch: 70, Batch: 200, D Loss: 1.2553086280822754, G Loss: 0.9909452199935913\n",
            "Epoch: 70, Batch: 300, D Loss: 1.2545037269592285, G Loss: 0.9225543737411499\n",
            "Epoch: 70, Batch: 400, D Loss: 1.283624291419983, G Loss: 0.8310193419456482\n",
            "Epoch: 70, Batch: 500, D Loss: 1.285139799118042, G Loss: 0.8068041205406189\n",
            "Epoch: 70, Batch: 600, D Loss: 1.2606256008148193, G Loss: 0.7469519376754761\n",
            "Epoch: 70, Batch: 700, D Loss: 1.2368640899658203, G Loss: 0.9037169814109802\n",
            "Epoch: 70, Batch: 800, D Loss: 1.2540347576141357, G Loss: 0.967717170715332\n",
            "Epoch: 70, Batch: 900, D Loss: 1.273587703704834, G Loss: 0.8713900446891785\n",
            "Epoch: 71, Batch: 0, D Loss: 1.2381939888000488, G Loss: 0.8380813002586365\n",
            "Epoch: 71, Batch: 100, D Loss: 1.2925769090652466, G Loss: 0.961079478263855\n",
            "Epoch: 71, Batch: 200, D Loss: 1.2623869180679321, G Loss: 1.0310074090957642\n",
            "Epoch: 71, Batch: 300, D Loss: 1.3624939918518066, G Loss: 0.8923126459121704\n",
            "Epoch: 71, Batch: 400, D Loss: 1.422380805015564, G Loss: 0.9147019982337952\n",
            "Epoch: 71, Batch: 500, D Loss: 1.3156461715698242, G Loss: 0.9609060287475586\n",
            "Epoch: 71, Batch: 600, D Loss: 1.2728526592254639, G Loss: 0.7241036891937256\n",
            "Epoch: 71, Batch: 700, D Loss: 1.2610721588134766, G Loss: 0.8929646611213684\n",
            "Epoch: 71, Batch: 800, D Loss: 1.3276740312576294, G Loss: 0.9811875820159912\n",
            "Epoch: 71, Batch: 900, D Loss: 1.2585269212722778, G Loss: 0.8969181776046753\n",
            "Epoch: 72, Batch: 0, D Loss: 1.2437539100646973, G Loss: 0.9405240416526794\n",
            "Epoch: 72, Batch: 100, D Loss: 1.3011144399642944, G Loss: 0.8155316710472107\n",
            "Epoch: 72, Batch: 200, D Loss: 1.2873499393463135, G Loss: 0.8008095622062683\n",
            "Epoch: 72, Batch: 300, D Loss: 1.351454496383667, G Loss: 1.0108896493911743\n",
            "Epoch: 72, Batch: 400, D Loss: 1.4249024391174316, G Loss: 0.8002718687057495\n",
            "Epoch: 72, Batch: 500, D Loss: 1.2816976308822632, G Loss: 0.7045261859893799\n",
            "Epoch: 72, Batch: 600, D Loss: 1.2454805374145508, G Loss: 0.887875497341156\n",
            "Epoch: 72, Batch: 700, D Loss: 1.176698923110962, G Loss: 0.8282448649406433\n",
            "Epoch: 72, Batch: 800, D Loss: 1.2328345775604248, G Loss: 0.8583564162254333\n",
            "Epoch: 72, Batch: 900, D Loss: 1.2526724338531494, G Loss: 0.8031339049339294\n",
            "Epoch: 73, Batch: 0, D Loss: 1.2947514057159424, G Loss: 0.9001473188400269\n",
            "Epoch: 73, Batch: 100, D Loss: 1.325805425643921, G Loss: 0.809038519859314\n",
            "Epoch: 73, Batch: 200, D Loss: 1.2810662984848022, G Loss: 0.7597071528434753\n",
            "Epoch: 73, Batch: 300, D Loss: 1.2561755180358887, G Loss: 0.8210031986236572\n",
            "Epoch: 73, Batch: 400, D Loss: 1.3405041694641113, G Loss: 0.8553028702735901\n",
            "Epoch: 73, Batch: 500, D Loss: 1.2643065452575684, G Loss: 0.8646525144577026\n",
            "Epoch: 73, Batch: 600, D Loss: 1.2515738010406494, G Loss: 0.7392739653587341\n",
            "Epoch: 73, Batch: 700, D Loss: 1.3180077075958252, G Loss: 1.0124528408050537\n",
            "Epoch: 73, Batch: 800, D Loss: 1.2963640689849854, G Loss: 0.8333357572555542\n",
            "Epoch: 73, Batch: 900, D Loss: 1.3233201503753662, G Loss: 0.9113840460777283\n",
            "Epoch: 74, Batch: 0, D Loss: 1.3073939085006714, G Loss: 0.786721408367157\n",
            "Epoch: 74, Batch: 100, D Loss: 1.3596181869506836, G Loss: 0.7798393368721008\n",
            "Epoch: 74, Batch: 200, D Loss: 1.2480345964431763, G Loss: 0.7294887900352478\n",
            "Epoch: 74, Batch: 300, D Loss: 1.266404390335083, G Loss: 0.9241551160812378\n",
            "Epoch: 74, Batch: 400, D Loss: 1.2862231731414795, G Loss: 0.9771371483802795\n",
            "Epoch: 74, Batch: 500, D Loss: 1.2615482807159424, G Loss: 0.8875834345817566\n",
            "Epoch: 74, Batch: 600, D Loss: 1.2211565971374512, G Loss: 0.8077346682548523\n",
            "Epoch: 74, Batch: 700, D Loss: 1.3506516218185425, G Loss: 0.9084991812705994\n",
            "Epoch: 74, Batch: 800, D Loss: 1.3560872077941895, G Loss: 0.8366248607635498\n",
            "Epoch: 74, Batch: 900, D Loss: 1.2753865718841553, G Loss: 0.9191797971725464\n",
            "Epoch: 75, Batch: 0, D Loss: 1.2245776653289795, G Loss: 0.7977768182754517\n",
            "Epoch: 75, Batch: 100, D Loss: 1.208483338356018, G Loss: 0.8516627550125122\n",
            "Epoch: 75, Batch: 200, D Loss: 1.4333677291870117, G Loss: 0.866212785243988\n",
            "Epoch: 75, Batch: 300, D Loss: 1.2453422546386719, G Loss: 0.8919516205787659\n",
            "Epoch: 75, Batch: 400, D Loss: 1.2386890649795532, G Loss: 1.026982069015503\n",
            "Epoch: 75, Batch: 500, D Loss: 1.41147780418396, G Loss: 0.8412307500839233\n",
            "Epoch: 75, Batch: 600, D Loss: 1.4189984798431396, G Loss: 0.9117537140846252\n",
            "Epoch: 75, Batch: 700, D Loss: 1.2292225360870361, G Loss: 0.891796350479126\n",
            "Epoch: 75, Batch: 800, D Loss: 1.418234944343567, G Loss: 1.114423155784607\n",
            "Epoch: 75, Batch: 900, D Loss: 1.2471623420715332, G Loss: 0.8574296236038208\n",
            "Epoch: 76, Batch: 0, D Loss: 1.3540091514587402, G Loss: 0.9380613565444946\n",
            "Epoch: 76, Batch: 100, D Loss: 1.3500046730041504, G Loss: 0.8398016691207886\n",
            "Epoch: 76, Batch: 200, D Loss: 1.3418008089065552, G Loss: 0.8128170371055603\n",
            "Epoch: 76, Batch: 300, D Loss: 1.284570336341858, G Loss: 0.9134376645088196\n",
            "Epoch: 76, Batch: 400, D Loss: 1.2511519193649292, G Loss: 0.842275857925415\n",
            "Epoch: 76, Batch: 500, D Loss: 1.236525297164917, G Loss: 0.9085462689399719\n",
            "Epoch: 76, Batch: 600, D Loss: 1.2517979145050049, G Loss: 0.8112252354621887\n",
            "Epoch: 76, Batch: 700, D Loss: 1.341315507888794, G Loss: 0.8484185338020325\n",
            "Epoch: 76, Batch: 800, D Loss: 1.3143115043640137, G Loss: 0.8262758851051331\n",
            "Epoch: 76, Batch: 900, D Loss: 1.2639553546905518, G Loss: 0.8572803139686584\n",
            "Epoch: 77, Batch: 0, D Loss: 1.3637847900390625, G Loss: 0.8854290246963501\n",
            "Epoch: 77, Batch: 100, D Loss: 1.1686317920684814, G Loss: 0.741303563117981\n",
            "Epoch: 77, Batch: 200, D Loss: 1.270349383354187, G Loss: 0.8550835251808167\n",
            "Epoch: 77, Batch: 300, D Loss: 1.21526038646698, G Loss: 0.867602527141571\n",
            "Epoch: 77, Batch: 400, D Loss: 1.2875553369522095, G Loss: 0.7855562567710876\n",
            "Epoch: 77, Batch: 500, D Loss: 1.3007166385650635, G Loss: 0.8290531635284424\n",
            "Epoch: 77, Batch: 600, D Loss: 1.286704421043396, G Loss: 0.9622952938079834\n",
            "Epoch: 77, Batch: 700, D Loss: 1.2069963216781616, G Loss: 0.812404990196228\n",
            "Epoch: 77, Batch: 800, D Loss: 1.1803861856460571, G Loss: 0.8343859314918518\n",
            "Epoch: 77, Batch: 900, D Loss: 1.270324468612671, G Loss: 0.7812373042106628\n",
            "Epoch: 78, Batch: 0, D Loss: 1.332794189453125, G Loss: 0.8455803394317627\n",
            "Epoch: 78, Batch: 100, D Loss: 1.3324097394943237, G Loss: 0.7877983450889587\n",
            "Epoch: 78, Batch: 200, D Loss: 1.375510811805725, G Loss: 0.7786531448364258\n",
            "Epoch: 78, Batch: 300, D Loss: 1.2811470031738281, G Loss: 0.9086282253265381\n",
            "Epoch: 78, Batch: 400, D Loss: 1.307072401046753, G Loss: 0.9198585152626038\n",
            "Epoch: 78, Batch: 500, D Loss: 1.3689744472503662, G Loss: 0.8617121577262878\n",
            "Epoch: 78, Batch: 600, D Loss: 1.318005084991455, G Loss: 0.7024269700050354\n",
            "Epoch: 78, Batch: 700, D Loss: 1.2329096794128418, G Loss: 0.8740304112434387\n",
            "Epoch: 78, Batch: 800, D Loss: 1.310976266860962, G Loss: 0.7862937450408936\n",
            "Epoch: 78, Batch: 900, D Loss: 1.221076250076294, G Loss: 0.953733503818512\n",
            "Epoch: 79, Batch: 0, D Loss: 1.3107869625091553, G Loss: 1.0165796279907227\n",
            "Epoch: 79, Batch: 100, D Loss: 1.1936399936676025, G Loss: 0.9446153044700623\n",
            "Epoch: 79, Batch: 200, D Loss: 1.2242226600646973, G Loss: 0.8760912418365479\n",
            "Epoch: 79, Batch: 300, D Loss: 1.3264033794403076, G Loss: 0.8423064947128296\n",
            "Epoch: 79, Batch: 400, D Loss: 1.314384937286377, G Loss: 0.922814130783081\n",
            "Epoch: 79, Batch: 500, D Loss: 1.286107063293457, G Loss: 0.8045095801353455\n",
            "Epoch: 79, Batch: 600, D Loss: 1.296241044998169, G Loss: 0.8068662881851196\n",
            "Epoch: 79, Batch: 700, D Loss: 1.2573965787887573, G Loss: 0.8887351751327515\n",
            "Epoch: 79, Batch: 800, D Loss: 1.253941535949707, G Loss: 0.8414927124977112\n",
            "Epoch: 79, Batch: 900, D Loss: 1.2579256296157837, G Loss: 0.9609413743019104\n",
            "Epoch: 80, Batch: 0, D Loss: 1.1827280521392822, G Loss: 0.7755889296531677\n",
            "Epoch: 80, Batch: 100, D Loss: 1.2119197845458984, G Loss: 0.8514981865882874\n",
            "Epoch: 80, Batch: 200, D Loss: 1.3610728979110718, G Loss: 0.7530671954154968\n",
            "Epoch: 80, Batch: 300, D Loss: 1.3038606643676758, G Loss: 0.7108460664749146\n",
            "Epoch: 80, Batch: 400, D Loss: 1.4005705118179321, G Loss: 0.8981543779373169\n",
            "Epoch: 80, Batch: 500, D Loss: 1.2175133228302002, G Loss: 0.867019534111023\n",
            "Epoch: 80, Batch: 600, D Loss: 1.2674121856689453, G Loss: 0.9393019080162048\n",
            "Epoch: 80, Batch: 700, D Loss: 1.3283030986785889, G Loss: 0.8979294896125793\n",
            "Epoch: 80, Batch: 800, D Loss: 1.257007360458374, G Loss: 0.7786341905593872\n",
            "Epoch: 80, Batch: 900, D Loss: 1.271690845489502, G Loss: 0.8144317865371704\n",
            "Epoch: 81, Batch: 0, D Loss: 1.3110566139221191, G Loss: 0.8234406113624573\n",
            "Epoch: 81, Batch: 100, D Loss: 1.1761430501937866, G Loss: 0.8047701716423035\n",
            "Epoch: 81, Batch: 200, D Loss: 1.2158799171447754, G Loss: 0.8478542566299438\n",
            "Epoch: 81, Batch: 300, D Loss: 1.3979179859161377, G Loss: 0.9804264903068542\n",
            "Epoch: 81, Batch: 400, D Loss: 1.290581464767456, G Loss: 0.8682807087898254\n",
            "Epoch: 81, Batch: 500, D Loss: 1.2686545848846436, G Loss: 0.7714840173721313\n",
            "Epoch: 81, Batch: 600, D Loss: 1.3203141689300537, G Loss: 0.7924646139144897\n",
            "Epoch: 81, Batch: 700, D Loss: 1.3240623474121094, G Loss: 0.820644199848175\n",
            "Epoch: 81, Batch: 800, D Loss: 1.2982295751571655, G Loss: 0.7959011197090149\n",
            "Epoch: 81, Batch: 900, D Loss: 1.168266773223877, G Loss: 0.8788444995880127\n",
            "Epoch: 82, Batch: 0, D Loss: 1.2692087888717651, G Loss: 0.9249460101127625\n",
            "Epoch: 82, Batch: 100, D Loss: 1.3470029830932617, G Loss: 0.8280264735221863\n",
            "Epoch: 82, Batch: 200, D Loss: 1.2845041751861572, G Loss: 0.6696311831474304\n",
            "Epoch: 82, Batch: 300, D Loss: 1.3475267887115479, G Loss: 0.8417243361473083\n",
            "Epoch: 82, Batch: 400, D Loss: 1.3315926790237427, G Loss: 0.8727218508720398\n",
            "Epoch: 82, Batch: 500, D Loss: 1.2223830223083496, G Loss: 0.834668755531311\n",
            "Epoch: 82, Batch: 600, D Loss: 1.4820079803466797, G Loss: 0.8998938798904419\n",
            "Epoch: 82, Batch: 700, D Loss: 1.272641897201538, G Loss: 0.9371400475502014\n",
            "Epoch: 82, Batch: 800, D Loss: 1.339845895767212, G Loss: 0.8043848276138306\n",
            "Epoch: 82, Batch: 900, D Loss: 1.251643419265747, G Loss: 0.8446978330612183\n",
            "Epoch: 83, Batch: 0, D Loss: 1.3235831260681152, G Loss: 0.910471498966217\n",
            "Epoch: 83, Batch: 100, D Loss: 1.2393083572387695, G Loss: 0.8898442387580872\n",
            "Epoch: 83, Batch: 200, D Loss: 1.2401775121688843, G Loss: 0.8275613784790039\n",
            "Epoch: 83, Batch: 300, D Loss: 1.2393070459365845, G Loss: 0.898565411567688\n",
            "Epoch: 83, Batch: 400, D Loss: 1.281447172164917, G Loss: 0.8769072890281677\n",
            "Epoch: 83, Batch: 500, D Loss: 1.309558629989624, G Loss: 0.8623616099357605\n",
            "Epoch: 83, Batch: 600, D Loss: 1.3166115283966064, G Loss: 0.8815746307373047\n",
            "Epoch: 83, Batch: 700, D Loss: 1.296184778213501, G Loss: 0.716407060623169\n",
            "Epoch: 83, Batch: 800, D Loss: 1.218057632446289, G Loss: 0.8795261979103088\n",
            "Epoch: 83, Batch: 900, D Loss: 1.3023736476898193, G Loss: 0.8655214905738831\n",
            "Epoch: 84, Batch: 0, D Loss: 1.3199985027313232, G Loss: 0.7644096612930298\n",
            "Epoch: 84, Batch: 100, D Loss: 1.259289264678955, G Loss: 0.8274219632148743\n",
            "Epoch: 84, Batch: 200, D Loss: 1.2537473440170288, G Loss: 0.8863114714622498\n",
            "Epoch: 84, Batch: 300, D Loss: 1.2518248558044434, G Loss: 0.7894461154937744\n",
            "Epoch: 84, Batch: 400, D Loss: 1.2563600540161133, G Loss: 0.8829418420791626\n",
            "Epoch: 84, Batch: 500, D Loss: 1.2695839405059814, G Loss: 0.8893439769744873\n",
            "Epoch: 84, Batch: 600, D Loss: 1.1922270059585571, G Loss: 0.8603858947753906\n",
            "Epoch: 84, Batch: 700, D Loss: 1.1147600412368774, G Loss: 0.8574894666671753\n",
            "Epoch: 84, Batch: 800, D Loss: 1.251861810684204, G Loss: 0.8947507739067078\n",
            "Epoch: 84, Batch: 900, D Loss: 1.1969447135925293, G Loss: 0.7784502506256104\n",
            "Epoch: 85, Batch: 0, D Loss: 1.3303793668746948, G Loss: 0.7839782238006592\n",
            "Epoch: 85, Batch: 100, D Loss: 1.2949798107147217, G Loss: 0.8233655095100403\n",
            "Epoch: 85, Batch: 200, D Loss: 1.3958070278167725, G Loss: 0.9368105530738831\n",
            "Epoch: 85, Batch: 300, D Loss: 1.2997980117797852, G Loss: 0.9007532596588135\n",
            "Epoch: 85, Batch: 400, D Loss: 1.4158955812454224, G Loss: 0.8211777210235596\n",
            "Epoch: 85, Batch: 500, D Loss: 1.3250406980514526, G Loss: 0.818719208240509\n",
            "Epoch: 85, Batch: 600, D Loss: 1.3608317375183105, G Loss: 0.7655782699584961\n",
            "Epoch: 85, Batch: 700, D Loss: 1.278444766998291, G Loss: 0.8289083242416382\n",
            "Epoch: 85, Batch: 800, D Loss: 1.2289650440216064, G Loss: 0.7936087846755981\n",
            "Epoch: 85, Batch: 900, D Loss: 1.3298306465148926, G Loss: 0.955469012260437\n",
            "Epoch: 86, Batch: 0, D Loss: 1.4013229608535767, G Loss: 0.9253383874893188\n",
            "Epoch: 86, Batch: 100, D Loss: 1.3297836780548096, G Loss: 0.8782405257225037\n",
            "Epoch: 86, Batch: 200, D Loss: 1.3303611278533936, G Loss: 0.8090949654579163\n",
            "Epoch: 86, Batch: 300, D Loss: 1.2752918004989624, G Loss: 0.8843261003494263\n",
            "Epoch: 86, Batch: 400, D Loss: 1.4060149192810059, G Loss: 0.860154926776886\n",
            "Epoch: 86, Batch: 500, D Loss: 1.2752195596694946, G Loss: 0.8396309018135071\n",
            "Epoch: 86, Batch: 600, D Loss: 1.2745001316070557, G Loss: 0.85706627368927\n",
            "Epoch: 86, Batch: 700, D Loss: 1.260806918144226, G Loss: 0.7903469204902649\n",
            "Epoch: 86, Batch: 800, D Loss: 1.2993974685668945, G Loss: 0.9463160037994385\n",
            "Epoch: 86, Batch: 900, D Loss: 1.2450816631317139, G Loss: 0.9029312133789062\n",
            "Epoch: 87, Batch: 0, D Loss: 1.3531408309936523, G Loss: 0.9266114830970764\n",
            "Epoch: 87, Batch: 100, D Loss: 1.211470127105713, G Loss: 0.7994787096977234\n",
            "Epoch: 87, Batch: 200, D Loss: 1.176039457321167, G Loss: 0.8445610404014587\n",
            "Epoch: 87, Batch: 300, D Loss: 1.251091480255127, G Loss: 0.8761696219444275\n",
            "Epoch: 87, Batch: 400, D Loss: 1.2703847885131836, G Loss: 0.9325286746025085\n",
            "Epoch: 87, Batch: 500, D Loss: 1.220487356185913, G Loss: 0.8686723113059998\n",
            "Epoch: 87, Batch: 600, D Loss: 1.2447518110275269, G Loss: 0.8920721411705017\n",
            "Epoch: 87, Batch: 700, D Loss: 1.244438648223877, G Loss: 0.7229229211807251\n",
            "Epoch: 87, Batch: 800, D Loss: 1.3637582063674927, G Loss: 0.9317110776901245\n",
            "Epoch: 87, Batch: 900, D Loss: 1.196658968925476, G Loss: 0.8194515109062195\n",
            "Epoch: 88, Batch: 0, D Loss: 1.2275805473327637, G Loss: 0.8758680820465088\n",
            "Epoch: 88, Batch: 100, D Loss: 1.3049192428588867, G Loss: 0.8885403871536255\n",
            "Epoch: 88, Batch: 200, D Loss: 1.303604006767273, G Loss: 0.9357253313064575\n",
            "Epoch: 88, Batch: 300, D Loss: 1.2792205810546875, G Loss: 0.9275118112564087\n",
            "Epoch: 88, Batch: 400, D Loss: 1.2820900678634644, G Loss: 0.9392489790916443\n",
            "Epoch: 88, Batch: 500, D Loss: 1.2795058488845825, G Loss: 0.7819178104400635\n",
            "Epoch: 88, Batch: 600, D Loss: 1.4259214401245117, G Loss: 0.8923521041870117\n",
            "Epoch: 88, Batch: 700, D Loss: 1.3235657215118408, G Loss: 0.8853724002838135\n",
            "Epoch: 88, Batch: 800, D Loss: 1.3531852960586548, G Loss: 0.8480318188667297\n",
            "Epoch: 88, Batch: 900, D Loss: 1.3330085277557373, G Loss: 0.9759935140609741\n",
            "Epoch: 89, Batch: 0, D Loss: 1.294986367225647, G Loss: 0.881999671459198\n",
            "Epoch: 89, Batch: 100, D Loss: 1.2384885549545288, G Loss: 0.8791093826293945\n",
            "Epoch: 89, Batch: 200, D Loss: 1.2160377502441406, G Loss: 0.8775202035903931\n",
            "Epoch: 89, Batch: 300, D Loss: 1.2183233499526978, G Loss: 0.8331244587898254\n",
            "Epoch: 89, Batch: 400, D Loss: 1.358893632888794, G Loss: 0.9390067458152771\n",
            "Epoch: 89, Batch: 500, D Loss: 1.2319965362548828, G Loss: 0.7767398953437805\n",
            "Epoch: 89, Batch: 600, D Loss: 1.2363313436508179, G Loss: 0.8832146525382996\n",
            "Epoch: 89, Batch: 700, D Loss: 1.3037166595458984, G Loss: 0.8352133631706238\n",
            "Epoch: 89, Batch: 800, D Loss: 1.2208049297332764, G Loss: 0.8336464762687683\n",
            "Epoch: 89, Batch: 900, D Loss: 1.3157827854156494, G Loss: 0.9785383939743042\n",
            "Epoch: 90, Batch: 0, D Loss: 1.2120673656463623, G Loss: 0.836089551448822\n",
            "Epoch: 90, Batch: 100, D Loss: 1.264732837677002, G Loss: 0.902690052986145\n",
            "Epoch: 90, Batch: 200, D Loss: 1.4067192077636719, G Loss: 0.9561333060264587\n",
            "Epoch: 90, Batch: 300, D Loss: 1.2853178977966309, G Loss: 0.9139879941940308\n",
            "Epoch: 90, Batch: 400, D Loss: 1.3092886209487915, G Loss: 0.8875178098678589\n",
            "Epoch: 90, Batch: 500, D Loss: 1.3475050926208496, G Loss: 0.8597718477249146\n",
            "Epoch: 90, Batch: 600, D Loss: 1.2336244583129883, G Loss: 0.7840559482574463\n",
            "Epoch: 90, Batch: 700, D Loss: 1.240992546081543, G Loss: 0.9389402270317078\n",
            "Epoch: 90, Batch: 800, D Loss: 1.2183897495269775, G Loss: 0.8406538367271423\n",
            "Epoch: 90, Batch: 900, D Loss: 1.245360255241394, G Loss: 0.8835117816925049\n",
            "Epoch: 91, Batch: 0, D Loss: 1.275334358215332, G Loss: 1.0374740362167358\n",
            "Epoch: 91, Batch: 100, D Loss: 1.349553108215332, G Loss: 0.8703879117965698\n",
            "Epoch: 91, Batch: 200, D Loss: 1.3535192012786865, G Loss: 0.8973545432090759\n",
            "Epoch: 91, Batch: 300, D Loss: 1.3323074579238892, G Loss: 0.8344606757164001\n",
            "Epoch: 91, Batch: 400, D Loss: 1.2487602233886719, G Loss: 0.9294096827507019\n",
            "Epoch: 91, Batch: 500, D Loss: 1.2663795948028564, G Loss: 0.8339108824729919\n",
            "Epoch: 91, Batch: 600, D Loss: 1.3424867391586304, G Loss: 0.8059074282646179\n",
            "Epoch: 91, Batch: 700, D Loss: 1.311865210533142, G Loss: 0.7948290705680847\n",
            "Epoch: 91, Batch: 800, D Loss: 1.2607176303863525, G Loss: 0.8877813816070557\n",
            "Epoch: 91, Batch: 900, D Loss: 1.3231265544891357, G Loss: 0.9166470766067505\n",
            "Epoch: 92, Batch: 0, D Loss: 1.3425774574279785, G Loss: 0.8673466444015503\n",
            "Epoch: 92, Batch: 100, D Loss: 1.2797307968139648, G Loss: 0.8765323758125305\n",
            "Epoch: 92, Batch: 200, D Loss: 1.1433753967285156, G Loss: 0.8228960633277893\n",
            "Epoch: 92, Batch: 300, D Loss: 1.2926907539367676, G Loss: 0.9409114122390747\n",
            "Epoch: 92, Batch: 400, D Loss: 1.3114733695983887, G Loss: 1.0178735256195068\n",
            "Epoch: 92, Batch: 500, D Loss: 1.2789664268493652, G Loss: 0.8629577159881592\n",
            "Epoch: 92, Batch: 600, D Loss: 1.2878460884094238, G Loss: 0.9714284539222717\n",
            "Epoch: 92, Batch: 700, D Loss: 1.257374882698059, G Loss: 0.9000186920166016\n",
            "Epoch: 92, Batch: 800, D Loss: 1.28395676612854, G Loss: 0.8322269320487976\n",
            "Epoch: 92, Batch: 900, D Loss: 1.3127683401107788, G Loss: 0.9089481830596924\n",
            "Epoch: 93, Batch: 0, D Loss: 1.253901720046997, G Loss: 0.8569292426109314\n",
            "Epoch: 93, Batch: 100, D Loss: 1.197061538696289, G Loss: 0.9263569712638855\n",
            "Epoch: 93, Batch: 200, D Loss: 1.3394598960876465, G Loss: 0.7827714681625366\n",
            "Epoch: 93, Batch: 300, D Loss: 1.2061436176300049, G Loss: 0.7791847586631775\n",
            "Epoch: 93, Batch: 400, D Loss: 1.3264501094818115, G Loss: 0.8683417439460754\n",
            "Epoch: 93, Batch: 500, D Loss: 1.2728688716888428, G Loss: 0.8460171818733215\n",
            "Epoch: 93, Batch: 600, D Loss: 1.2698943614959717, G Loss: 0.7416315078735352\n",
            "Epoch: 93, Batch: 700, D Loss: 1.2636531591415405, G Loss: 0.9801100492477417\n",
            "Epoch: 93, Batch: 800, D Loss: 1.3469786643981934, G Loss: 0.8363110423088074\n",
            "Epoch: 93, Batch: 900, D Loss: 1.2227783203125, G Loss: 0.9611837863922119\n",
            "Epoch: 94, Batch: 0, D Loss: 1.378868579864502, G Loss: 0.8093119859695435\n",
            "Epoch: 94, Batch: 100, D Loss: 1.2837870121002197, G Loss: 0.9127767086029053\n",
            "Epoch: 94, Batch: 200, D Loss: 1.2934552431106567, G Loss: 0.7502601742744446\n",
            "Epoch: 94, Batch: 300, D Loss: 1.2503900527954102, G Loss: 0.7775445580482483\n",
            "Epoch: 94, Batch: 400, D Loss: 1.282647967338562, G Loss: 0.8568498492240906\n",
            "Epoch: 94, Batch: 500, D Loss: 1.2549636363983154, G Loss: 0.7184740304946899\n",
            "Epoch: 94, Batch: 600, D Loss: 1.3095732927322388, G Loss: 0.7342042922973633\n",
            "Epoch: 94, Batch: 700, D Loss: 1.3125938177108765, G Loss: 0.7679093480110168\n",
            "Epoch: 94, Batch: 800, D Loss: 1.3745800256729126, G Loss: 0.8466047644615173\n",
            "Epoch: 94, Batch: 900, D Loss: 1.3096641302108765, G Loss: 0.843178927898407\n",
            "Epoch: 95, Batch: 0, D Loss: 1.2933814525604248, G Loss: 0.8562613129615784\n",
            "Epoch: 95, Batch: 100, D Loss: 1.2850701808929443, G Loss: 0.8269223570823669\n",
            "Epoch: 95, Batch: 200, D Loss: 1.2205959558486938, G Loss: 0.8855208158493042\n",
            "Epoch: 95, Batch: 300, D Loss: 1.2837648391723633, G Loss: 0.8615409731864929\n",
            "Epoch: 95, Batch: 400, D Loss: 1.382080078125, G Loss: 0.8462619781494141\n",
            "Epoch: 95, Batch: 500, D Loss: 1.2908666133880615, G Loss: 0.8728181719779968\n",
            "Epoch: 95, Batch: 600, D Loss: 1.2517273426055908, G Loss: 0.8710175156593323\n",
            "Epoch: 95, Batch: 700, D Loss: 1.332761287689209, G Loss: 0.7628170847892761\n",
            "Epoch: 95, Batch: 800, D Loss: 1.3860706090927124, G Loss: 0.8544326424598694\n",
            "Epoch: 95, Batch: 900, D Loss: 1.2474064826965332, G Loss: 0.8690130710601807\n",
            "Epoch: 96, Batch: 0, D Loss: 1.2424533367156982, G Loss: 0.8642469644546509\n",
            "Epoch: 96, Batch: 100, D Loss: 1.3165979385375977, G Loss: 0.9476162195205688\n",
            "Epoch: 96, Batch: 200, D Loss: 1.334743857383728, G Loss: 0.8488268256187439\n",
            "Epoch: 96, Batch: 300, D Loss: 1.3669570684432983, G Loss: 0.7836183309555054\n",
            "Epoch: 96, Batch: 400, D Loss: 1.19921875, G Loss: 0.799956738948822\n",
            "Epoch: 96, Batch: 500, D Loss: 1.3149564266204834, G Loss: 0.9067953824996948\n",
            "Epoch: 96, Batch: 600, D Loss: 1.1774044036865234, G Loss: 0.9014960527420044\n",
            "Epoch: 96, Batch: 700, D Loss: 1.336057424545288, G Loss: 0.7155254483222961\n",
            "Epoch: 96, Batch: 800, D Loss: 1.3395187854766846, G Loss: 0.8214892148971558\n",
            "Epoch: 96, Batch: 900, D Loss: 1.3819453716278076, G Loss: 0.8916956782341003\n",
            "Epoch: 97, Batch: 0, D Loss: 1.2653467655181885, G Loss: 0.8212520480155945\n",
            "Epoch: 97, Batch: 100, D Loss: 1.3233039379119873, G Loss: 0.7447982430458069\n",
            "Epoch: 97, Batch: 200, D Loss: 1.2344982624053955, G Loss: 0.9266685843467712\n",
            "Epoch: 97, Batch: 300, D Loss: 1.2853119373321533, G Loss: 0.8884638547897339\n",
            "Epoch: 97, Batch: 400, D Loss: 1.1916694641113281, G Loss: 0.8533769845962524\n",
            "Epoch: 97, Batch: 500, D Loss: 1.2161307334899902, G Loss: 0.8381401896476746\n",
            "Epoch: 97, Batch: 600, D Loss: 1.2640454769134521, G Loss: 0.8151771426200867\n",
            "Epoch: 97, Batch: 700, D Loss: 1.1630830764770508, G Loss: 0.7835204005241394\n",
            "Epoch: 97, Batch: 800, D Loss: 1.313753366470337, G Loss: 0.9037073850631714\n",
            "Epoch: 97, Batch: 900, D Loss: 1.2624539136886597, G Loss: 0.7816342711448669\n",
            "Epoch: 98, Batch: 0, D Loss: 1.211404800415039, G Loss: 0.7648365497589111\n",
            "Epoch: 98, Batch: 100, D Loss: 1.2147244215011597, G Loss: 0.9324384331703186\n",
            "Epoch: 98, Batch: 200, D Loss: 1.3644623756408691, G Loss: 0.907986581325531\n",
            "Epoch: 98, Batch: 300, D Loss: 1.344541311264038, G Loss: 0.8684684634208679\n",
            "Epoch: 98, Batch: 400, D Loss: 1.3082587718963623, G Loss: 0.8092442750930786\n",
            "Epoch: 98, Batch: 500, D Loss: 1.3941999673843384, G Loss: 0.7886304259300232\n",
            "Epoch: 98, Batch: 600, D Loss: 1.2891490459442139, G Loss: 0.8816241025924683\n",
            "Epoch: 98, Batch: 700, D Loss: 1.394770622253418, G Loss: 0.7794874906539917\n",
            "Epoch: 98, Batch: 800, D Loss: 1.2414803504943848, G Loss: 0.8308220505714417\n",
            "Epoch: 98, Batch: 900, D Loss: 1.3868274688720703, G Loss: 0.8709656000137329\n",
            "Epoch: 99, Batch: 0, D Loss: 1.1512819528579712, G Loss: 0.8717196583747864\n",
            "Epoch: 99, Batch: 100, D Loss: 1.3145983219146729, G Loss: 0.8570533394813538\n",
            "Epoch: 99, Batch: 200, D Loss: 1.3136558532714844, G Loss: 1.0082156658172607\n",
            "Epoch: 99, Batch: 300, D Loss: 1.3153796195983887, G Loss: 0.9114041924476624\n",
            "Epoch: 99, Batch: 400, D Loss: 1.295257806777954, G Loss: 0.8729448914527893\n",
            "Epoch: 99, Batch: 500, D Loss: 1.305568814277649, G Loss: 0.8531123399734497\n",
            "Epoch: 99, Batch: 600, D Loss: 1.2751758098602295, G Loss: 0.9226282238960266\n",
            "Epoch: 99, Batch: 700, D Loss: 1.3242785930633545, G Loss: 0.8271123170852661\n",
            "Epoch: 99, Batch: 800, D Loss: 1.3168766498565674, G Loss: 0.9122123718261719\n",
            "Epoch: 99, Batch: 900, D Loss: 1.2513000965118408, G Loss: 0.8959906101226807\n"
          ]
        }
      ],
      "source": [
        "# 데아터셋을 100번 돌며 학습한다.\n",
        "\n",
        "# 시각화를 위해 사용할 고정된 노이즈 벡터\n",
        "fixed_z = Variable(torch.randn((5 * 5, 100)))  # We will create 25 images\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    # 한 번에 batch_size만큼 데이터를 가져온다.\n",
        "    for i, (real_data, _) in enumerate(dataloader):\n",
        "        batch_size = real_data.size(0)\n",
        "    \n",
        "    # 데이터를 파이토치의 변수로 변환한다.\n",
        "        real_data = Variable(real_data)\n",
        "\n",
        "        # ### ### ### 구분자 학습시키기 ### ### ### #\n",
        "        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n",
        "        # 정답지에 해당하는 변수를 만든다.\n",
        "\n",
        "        target_real = Variable(torch.ones(batch_size, 1))\n",
        "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
        "\n",
        "        # 진짜 이미지를 구분자에 넣는다.\n",
        "        D_result_from_real = D(real_data)\n",
        "\n",
        "        # 구분자의 출력 값이 정답지인 1에서 멀수록 Loss가 높아진다.\n",
        "        D_loss_real = criterion(D_result_from_real, target_real)\n",
        "\n",
        "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
        "        z = Variable(torch.randn((batch_size, 100)))\n",
        "\n",
        "        # 생성자로 가짜 이미지를 생성한다.\n",
        "        fake_data = G(z)\n",
        "\n",
        "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
        "        D_result_from_fake = D(fake_data)\n",
        "\n",
        "        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n",
        "        D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
        "\n",
        "        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n",
        "        D.zero_grad()\n",
        "\n",
        "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
        "        D_loss.backward()\n",
        "\n",
        "        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n",
        "        D_optimier.step()\n",
        "        \n",
        "        # ### ### ### 생성자 학습시키기 ### ### ### 성\n",
        "        \n",
        "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
        "        z = Variable(torch.randn((batch_size, 100)))\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "\n",
        "        # 생성자로 가짜 이미지를 생성한다.\n",
        "        fake_data = G(z)\n",
        "\n",
        "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
        "        D_result_from_fake = D(fake_data)\n",
        "\n",
        "        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 Loss가 높아진다.\n",
        "        G_loss = criterion(D_result_from_fake, target_real)\n",
        "\n",
        "        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n",
        "        G.zero_grad()\n",
        "\n",
        "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
        "        G_loss.backward()\n",
        "\n",
        "        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n",
        "        G_optimizer.step()\n",
        "\n",
        "\n",
        "        if i % 500 == 0:  # Adjust the interval to your preference\n",
        "            G.eval()  # Set the generator to evaluation mode\n",
        "            save_image(G(fixed_z).view(-1, 1, 28, 28),\n",
        "                       f'generated_samples/epoch_{epoch}_batch_{i}.png',\n",
        "                       nrow=5, normalize=True)  # Save the generated images\n",
        "            G.train()  # Set the generator back to training mode\n",
        "\n",
        "        if i % 100 == 0:  # Adjust the interval to your preference\n",
        "            print(f\"Epoch: {epoch}, Batch: {i}, D Loss: {D_loss.item()}, G Loss: {G_loss.item()}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "1 - GAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
